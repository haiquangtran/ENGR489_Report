\chapter{Evaluation}\label{C:evaluation}

\section{Method?}

\section{User Data}

% \cite{martin2009recsys}
% Examples of this problem
% include the lack of standard treatment of items for which
% the recommender is unable to make a prediction. The broader
% goal of user-centered holistic evaluation, including A/B testing
% of the short- and long-term effects of recommendation differences,
% is still met by only a few research groups and companies
% that have the live systems and resources for such evaluation.

% Deploying innovative recommenders is still too hard, and there is a substantial need for research platforms where innovations can be tested without first building up a community of thousands of users. 

Show proportions of data

\section{Limitations}

\section{Issues}

\section{Offline Evaluation}

\subsection{Data Partitioning}

\subsection{Offline Metrics}

\subsubsection{Binary List}

\subsubsection{Precision \& Recall}

\subsubsection{Classification Accuracy}

\subsubsection{ROC Curves}

\subsubsection{Thresholding}

\section{Online Evaluation}


\section{Discussion}


