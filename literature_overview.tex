
\section{Literature Review}

FROM \cite{schafer2007collaborative}


The toolbox of recommender techniques has also grown beyond
collaborative filtering to include content-based approaches based on
information retrieval, bayesian inference, and case-based reasoning
methods
TODO: [132, 139].

These methods consider the actual content or attributes of the items to be recommended instead of or in addition to user rating patterns. 

Hybrid recommender systems [24] have also emerged as various recommender strategies have mature, combining multiple algorithms into composite systems that ideally build on the strengths of their component algorithms. Collaborative filtering, however,
has remained an effective approach, both alone and hybridized
with content-based approaches.

Research on recommender algorithms garnered significant attention in 2006 when Netflix launched the Netflix Prize to improve the state of movie recommendations. The objective of this competition was to build a recommender algorithm that could beat their internal CineMatch algorithm in offline tests by 10\%. It sparked a flurry of activity, both in academia and amongst hobbyists. The \$1 million prize demonstrates value that vendors place on accurate recommendations. 

\section{survey}
\cite{survey}
LOOK AT THIS MORE - REALLY GOOD

CONTAINS EVERYTHING. Can reference for problems of CF, different CF techniques etc.

+ As one of the most successful approaches to building recommender systems, collaborative filtering (CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users.

+ main challenges, sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc.

+ Memory based, model-based, hybrid based

+ The fundamental assumption of CF is that if users  and  rate  items similarly, or have similar behaviors (e.g., buying, watching, listening), and hence will rate or act on other items similarly \cite{goldberg}

+ neighborhood/memory based deployed in amazon.com, and Barnes and Noble, because easy-to-implement, and highly effective. [6,7]

+ Limitations of memory-based CF,
 - similarity values based on common items, thus unreliable when data are sparse, and common items are therefore few. 
 
 To achieve better prediction performance and overcome shortcomings of memory-based CF algorithms, model-based CF approaches have been investigated.
 
 + Model-based CF techniques (Section 4) use the pure rating data to estimate or learn a model to make predictions [9]. Well-known model-based CF techniques include Bayesian belief nets (BNs) CF models [9–11], clustering CF models [12, 13], and latent semantic CF models [7].
 
 + Content based filtering make recommendations by analyzing content of textual information and finding regularities in the content. 

+ DIFFERENCE:  CF only uses the user-item ratings data to make predictions and recommendations, while content-based recommender systems rely on the features of users and items for predictions [15].

+ Hybrid CF techniques, such as the content-boosted CF algorithm [16] and Personality Diagnosis (PD) [17], combine CF and content-based techniques, hoping to avoid the limitations of either approach and thereby improve recommendation performance (Section 5).

+ User-based top- recommendation algorithms firstly identify the  most similar users (nearest neighbors) to the active user using the Pearson correlation or vector-space model [9, 27]

+ Item based were developed to address scalability problem of user based recommendations. 


\section{dimension}
\cite{dimension} 
\citeyear{dimension}
+ Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with the number of customers and products
+ New recommender system
technologies are needed that can quickly produce
high quality recommendations, even for very largescale
problems

+ Collaborative filtering is the most successful
recommender system technology to date, and is used
in many of the most successful recommender systems
on the Web, including those at Amazon.com and CDnow.com. 

+ neighborhood methods rely upon exact matches that cause algorithms to scarifice recommender system coverage. many customers have no correlation at all. Accordingly, preason NN may be unablet o make many product recommendations for a particular user. (reduced coverage) due to sparse ratings of neighbors. accuracy of recommendations may be poor because fairly little ratings data can be included. 

+ neighbourhood methods require computation that grows with number of customers and products. With millions of customers and prdoucts, typical system will suffer scalability problems. 

+ Synonym problem: different product names can refer to similar objects. Discovery of latent relationship from database may potentially solve synonym problem in recommender systems. 

+ Weakness of Pearson nearest neighbour for large sparse databases led us to explore alternative recommender systems. 


+ Attempts to address sparsity: incorporating semi-intelligent filtering agents into system. These agents evaluated and rated each product, using syntactic features. By providing a dense ratings set, they helped alleviate coverage and improved quality. However, did not address poor relationships among like minded but sparse-rating customers. 

+ Latent Semantic Indexing (LSI) to reduce dimensionality of customer-product ratings matrix. LSI is a dimensionality reduction technique that has been widely used in information retrieval (IR) to
solve the problems of synonymy and polysemy
(Deerwester et al. 1990).
By reducing the dimensionality
of the product space, we can increase density and
thereby find more ratings. 

+ LSI uses SVD as its underlying matrix factorization lagorihtm, maps incely into CF. 

+ The SVD-based approach was
consistently worse than traditional collaborative
filtering in se of an extremely sparse e-commerce
dataset.

+ However, the SVD-based approach
produced results that were better than a traditional
collaborative filtering algorithm some of the time in
the denser MovieLens data set

+ this technique leads to very fast online performance, requiring just a few simple arithmetic operations for recommendation. 
+ Computing SVD is expensive but can be done offline. 


\section{Itembased}
\cite{itembased}
\citeyear{itembased}|
Summary: 
Analyzes different item-based recommendation generation algorithms. Look into different techniques for computing item-item similarities (e.g. item-item correlation vs. cosine similarities between item vectors) and different techniques for obtaining recommendations from them (e.g. weighted sum vs regression model). Compare with basic k-nearest neighbour approach. Results suggest item-based algorithms provide dramatically better performance than user-based algorithms, while at the same time providing better quality than he best available user-based algorithms. 

+ user based producing high quality recommendations, performing many recommendations per second for millions of users and items and achieving high coverage in the face of data sparsity. In traditional CF, the amount of work increases with the number of participants in the system. 

+ New recommender system technologies are needed that can quickly produce high quality recommendations, even for very large-scale problems. To address these issues we explore item based CF. 

+ Bayesian networks are practical for environments which knowledge of user preferences changes slowly with respect to the time needed to build the model but are not suitable for environments in which user preference models must be updated rapidly or frequently.

+clustering techniques work by identifying groups f users who appear to have similar preferences. predictions for an individual can be made by averaging opinions of other users in that cluster. Clustering techniques usually produce less-personal recommendations than other methods, and often worse accuracy than nearest neighbour algorithms. [6] However, performance can be very good, since size of group that must be analyzed is much smaller. Can also be first step to shrink candidate set in a nearest neighbour algorithm. preclustering may be a worthwhile trade-off between accuracy and throughput. 
+Model approaches - don't have to compute similarity measures to form neighbourhoods so tend to produce faster recommendations. Looked at a model approach to pre-compute item-item similarities

+ Horting is graph-based technique which nodes are users, and edges between nodes indicate degree of similarity between two users [1]. 

+ Sparsity problem in Recommender system has been addressed in [23,11]  


\section{Tapestry}
\cite{goldberg1992using}
\citeyear{goldberg1992using}|

Summary: 
The novelty of Tapestry lies in its support for collaborative
filtering. Users are encouraged to annotate documents,
and these annotations can then be used for filtering. We
envision two types of readers for various classes of
documents. Eager readers will read all the documents in
the class in order to get immediate access. More casual
readers will wait for the eager readers to annotate, and
read documents based on their reviews. Experience with
NetNews suggests that there will not be a lack of readers
willing to be ’eager’ annotators.

\section{schafer}
\cite{schafer2007collaborative}
\citeyear{schafer2007collaborative}

The differing personalities exhibited by different recommender
algorithms show that recommendation is not a one-size-
fits-all problem. Specific tasks, information needs, and item domains
represent unique problems for recommenders, and design and evaluation
of recommenders needs to be done based on the user tasks to
be supported

This paper discusses a wide variety of the choices available and their implications to the important issues underlying recommenders and current best practices for addressing these issues. 

* Similarly,
nearly all rating data sets are strongly biased toward high ratings,
because users are careful to only choose to consume items
they suspect they will like.

+Item–item collaborative filtering, also called item-based collaborative
filtering, takes a major step in this direction and is one of
the most widely deployed collaborative filtering techniques today

+In a system that has more
users than items, it allows the neighborhood-finding to be amongst the
smaller of the two dimensions, but this is a small gain. It provides
major performance gains by lending itself well to pre-computing the
similarity matrix.

\section{scalable}
\cite{scalable}
\citeyear{scalable}

+ enhance neighborhood-based approach leading to substantial improvement of prediction accuracy, without meaningful increase in running time by removing global effects from data to make ratings more comparable, improving interpolation accuracy. Secondly, show how to simultaneously derive interpolation weights for all nearest neighbors, unlike previous approaches where each weight is computed separately. 

+ Efficient computation of an item-item neighborhoodbased
method requires precomputing certain values associated
with each item-item pair for rapid retrieval.

+ Common choices are the Pearson correlation coefficient and the closely related cosine similarity. Methods also differ by how they normalize or center data prior to activating interpolation rule (1) or (2).

+ Sarwar et al. [15] found that item-oriented approaches deliver better
quality estimates than user-oriented approaches while allowing
more efficient computations.

+ Neighborhood-based methods became very popular because
they are intuitive and relatively simple to implement.
In particular, they do not require tuning many parameters
or an extensive training stage. They also provide a concise
and intuitive justification for the computed predictions.

+ Standard neighborhood method concerns:
- similarity function directly defines interpolation weights is arbitary. Suppose that a particular item is predicted
perfectly by a subset of the neighbors. In that
case, we would want the predictive subset to receive
all the weight, but that is impossible for bounded similarity
scores like the Pearson correlation coefficient

+ Previous neighborhood methods do not account for interactions among neighbors. For example,
suppose that our items are movies, and the neighbors
set contains three movies that are highly correlated
with each other (e.g., sequelssuch as “Lord of the
Rings 1–3”). An algorithm that ignores the similarity
of the three movies when determining their interpolation
weights, may end up essentially triple counting the
information provided by the group.

+ By definition, the interpolation weights sum to one, which may cause overfitting. '

+ In recent work [2], we overcame most of these shortcomings,
but the resulting algorithm was several orders
of magnitude slower than previous neighborhood-methods,
thereby limiting its practical applicability.

\section{Toward}
\cite{toward}
\citeyear{toward}

+ overview of content-based, collaborative, and hybrid recommendation approaches and limitations. 

+ Content-based recommendations: user will be recommended items similar to the ones the user preferred in the past. Tries to understand the commonalities among the movies that the user has rated highly in the past (specific actors, directors, genres, subject matter etc). Then, only the movies that have a high degree of similarity to whatever the user's preferences are would be recommended. 

+ Problems with content-based recommendations.
    - Limited by features that are explicitly associated with objects. Hard to extract features or have to manually put them in.
    - Cannot tell difference between good and bad items because contain same feature set
    - Overspecialization : limited to being recommended items that are similar to those already rated. 
    - User should be presented with a "range of options" (Diversity is important in recommender systems) 
    - Need to rate sufficient number of items before it can understand your preferences. 

+ collaborative recommendations: user will be recommended items that people with similar tastes and preferences liked in the past; Unlike content-based recommendation methods, collaborative recommender systems (or collaborative filtering systems) try to predict the utility of items for a particular user based on the items previously rated by other users.

+ hybrid approaches: methods combine collaborative and content-based methods. 

+ rating-based recommenders since it constitutes the most popular approach to recommender system


+ Grundy system would build individual user models
and use them to recommend relevant books to each user.
Later on, the Tapestry system relied on each user to
identify like-minded users manually [38]. GroupLens [53],
[86], Video Recommender [45], and Ringo [97] were the
first systems to use collaborative filtering algorithms to
automate prediction

+One problem with using the weighted
sum, as in (10b), is that it does not take into account the fact
that different users may use the rating scale differently. The
adjusted weighted sum, shown in (10c), has been widely
used to address this limitation. In this approach, instead of
using the absolute values of ratings, the weighted sum uses
their deviations from the average rating of the corresponding
user.

+ Memory based is using heuristics to find similarity between items or users

+ Model based is using probabilistic approach where unknown ratings will be predicted to users based on their previous rating patterns. cluster models and Bayesian networks. 
Clustered models cluster like-minded users into classes. The number of classes and parameters of the model are learned from the data.
Bayesian network represents each item in domain as a node in the network. Each state of the node correspond to possible rating values for each item. Both structure of network and the conditional probabilities are learned from the data. Limitations of this approach is that each user can be custered into a single cluster, whereas some recommendation applications may benefit from the ability to cluster users into several categories at once. For example,
in a book recommendation application, a user may be
interested in one topic (e.g., programming) for work
purposes and a completely different topic (e.g., fishing)
for leisure.based on a model
learned from the underlying

+ model-based methods outperform memory-based
approaches in terms of accuracy of recommendations.


+There have been several other model-based collaborative
recommendation approaches proposed in the literature. A
statistical model for collaborative filtering was proposed in
[105], and several different algorithms for estimating the
model parameters were compared, including K-means
clustering and Gibbs sampling. Other collaborative filtering
methods include a Bayesian model [20], a probabilistic
relational model [37], a linear regression [91], and a
maximum entropy model [75]. More recently, a significant
amount of research has been done in trying to model the
recommendation process using more complex probabilistic
models. For instance, Shani et al. [96] view the recommendation
process as a sequential decision problem and
propose using Markov decision processes (a well-known
stochastic technique for modeling sequential decisions) for
generating recommendations. Other probabilistic modeling
techniques for recommender systems include probabilistic
latent semantic analysis [47], [48] and a combination of
multinomial mixture and aspect models using generative
semantics of Latent Dirichlet Allocation [64]. Similarly, Si
and Jin [99] also use probabilistic latent semantic analysis to
propose a flexible mixture model that allows modeling the
classes of users and items explicitly with two sets of latent
variables


+Furthermore, Kumar et al. [55] use a simple
probabilistic model to demonstrate that collaborative filtering
is valuable with relatively little data on each user, and
that, in certain restricted settings, simple collaborative
filtering algorithms are almost as effective as the best
possible algorithms in terms of utility.

+The pure collaborative recommender systems do not
have some of the shortcomings that content-based systems
have. In particular, since collaborative systems use other
users’ recommendations (ratings), they can deal with any
kind of content and recommend any items, even the ones
that are dissimilar to those seen in the past

+COLLABORATIVE FILTERING PROBLEMS

 + New User Problem
    must learn user's preferences from ratings that the user gives. 
    Several approaches have been proposed to address this pobrlem, most being hybrid approaches. These techniques use strategies that     are based on item popularity, item entropy, user personalization, and
    combinations of the above [83], [109].
    
 + New Item Problem
until the new item is
rated by a substantial number of users, the recommender
system would not be able to recommend it

+ Sparsity

In any recommender system, the number of ratings already
obtained is usually very small compared to the number of
ratings that need to be predicted. Effective prediction of
ratings from a small number of examples is important. Also,
the success of the collaborative recommender system
depends on the availability of a critical mass of users. 

+ TO OVERCOME SPARSITY
- use user profile information when calculating user similarity. That is, two users could be considered similar not only if
they rated the same movies similarly, but also if they belong to the same demographic segment. For example, [76] uses the gender, age, area code, education, and employment
information of users in the restaurant recommendation
application
- A different
approach for dealing with sparse rating matrices was used
in [11], [90], where a dimensionality reduction technique,
Singular Value Decomposition (SVD), was used to reduce
the dimensionality of sparse ratings matrices


+ HYBRID APPROACHES
which
helps to avoid certain limitations of content-based and
collaborative systems [8], [9], [21], [76], [94], [100], [105].

1. implementing collaborative and content-based
methods separately and combining their predictions,
2. incorporating some content-based characteristics
into a collaborative approach,
3. incorporating some collaborative characteristics into
a content-based approach, and
4. constructing a general unifying model that incorporates
both content-based and collaborative
characteristics.