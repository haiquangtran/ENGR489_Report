\chapter{Background \& Discussion}\label{C:background}

As more recommender systems emerge, value is seen in the potential to increase sales from recommendations - customers may purchase suggested items they otherwise may not have discovered themselves \cite{schafer2007collaborative}. In the late 1990's, the most popular recommender system was Amazon.com, collecting user purchase history, browsing history, and recently viewed items to recommend items that the user may buy \cite{schafer2007collaborative}. Other recommender systems consisted of Jester \cite{goldberg} for jokes, and Ringo \cite{ringo} for music. Since then, there have been many recommendation techniques that have been explored. 

This chapter explains the background and literature of recommender system techniques. It firstly explores content-based filtering, and then explores collaborative filtering.

\section{Content-based Filtering}

Content-based filtering (CBF) has been widely used in recommender systems \cite{koren2011, hybrid,toward, bogers2009collaborative}. CBF focuses on recommendations to users based on item attributes that correspond to attributes a user has explicitly or implicitly indicated they prefer \cite{koren2011}. Users can explicitly rate item attributes to build a user profile, or machine-learning techniques such as naive Bayes \cite{koren2011, bogers2009collaborative} can implicitly learn the attributes of users to build a user profile. Recommendations are then provided based on these user profiles. Pandora \cite{pandora, howepandora}, a music streaming station, automatically plays recommended songs users may enjoy when they positively rate songs as ``Liked". The recommendations are based on approximately 400 attributes of a song, and are implicitly learnt \cite{howepandora}.

However, content-based filtering is prone to recommending a small subset of items, since it recommends items based on attributes, leading to overspecialisation and thus, restricting variety \cite{toward}. Another problem of content-based filtering is that it may be difficult to extract features (from items \& users), and manually labelling these features may be impractical \cite{toward}.

Traditional collaborative filtering techniques address these concerns since attributes are ignored, and recommendations are based on actions of other users. This provides a range of recommendations that are not restricted to specific attributes \cite{koren2009matrix}. Although collaborative filtering is considered to be one of the most successful approaches to recommender systems \cite{survey, toward}, they suffer from the problem of data sparsity \cite{toward, survey, itembased, koren2009matrix, koren2011, dimension}. Data sparsity is when only a small subset of user ratings on items are recorded, leading to an insufficient number of ratings to produce accurate recommendations. Data sparsity specifically tends to appear in the ``Cold Start" problem, where new items or new users are entered into the system, but insufficient information is supplied to produce accurate recommendations since recommendations are based on common items or users \cite{survey}. In these scenarios, content-based filtering is superior to collaborative filtering because it does not rely on the opinion of other users \cite{koren2009matrix}.

For these reasons, content-based filtering is not addressed in detail, but the basic idea of content-based filtering is explained since hybrid collaborative filtering approaches generally utilise content-based filtering to increase the recommendation accuracy \cite{survey}. An implementation of hybrid collaborative filtering is implemented in this project.  

\section{Collaborative Filtering}

The term ``collaborative filtering" (CF) was first introduced in \citeyear{goldberg1992using} by \citeauthor{goldberg1992using} \cite{goldberg1992using} to describe the technique used in Tapestry, one of the earliest known recommender systems \cite{koren2009matrix,  goldberg1992using, itembased, survey}.

Collaborative filtering is a popular technique that has been successfully used in recommender systems \cite{itembased, schafer2007collaborative, survey}. The main idea behind collaborative filtering is that rating behaviours of others can be used to predict items the active user will be interested in. Intuitively, this algorithm stems from the assumption that users having similar opinions on items they both rated, tend to have similar tastes, resulting in similar opinions about other items \cite{schafer2007collaborative}. For example, consider a friend with whom you share common food interests. Perhaps you both like eating chicken sandwiches. Since you both have similar tastes, your friend may then be able to suggest another food dish you might enjoy such as Thai Green Curry. Collaborative filtering uses the same concept, but usually on a larger scale. 

\begin{figure}
\centering
\includegraphics[scale=0.7]{images/User-Item}
\caption{A user-item ratings matrix containing scaled ratings from 1-5. Typically the user-item ratings matrix is sparsely filled, since there will be missing entries where users have not rated items. The missing entries are represented by the question marks (?) in the matrix. }
\label{fig:matrix}
\end{figure}

Users express preferences for a variety of items through ratings that can be in various forms such as 1-5 star ratings, or binary scale ratings such as likes/dislikes. These ratings are typically represented as a (User, Item, Rating) tuple. These ratings are represented in a matrix, where rows of the matrix are users, columns of the matrix are items, and ratings are represented inside the matrix as shown in Figure \ref{fig:matrix}. Since the matrix contains sets of rating tuples, the (User, Item) pairs will exist where a user has not yet rated the item, thus the matrix is usually sparse.

Since the matrix can be sparse, collaborative filtering can struggle when there are not enough ratings to provide accurate recommendations to users. Sparse ratings can also lead to another issue called the ``Cold Start" problem, where ratings for new items or new users have not yet been collected. In this context, content-based filtering is superior \cite{koren2009matrix}. However, the advantage of collaborative filtering is that it does not require domain knowledge of items or users to provide recommendations. It focuses on the previous behaviours of users and their rating history, providing recommendations that are generally more accurate than content based filtering once the active user has rated enough items \cite{koren2009matrix, schafer2007collaborative}. 

Using the matrix, a list of top N recommendations is given to users representing items they are predicted to prefer, computed through collaborative filtering techniques. For example, the active user may be recommended 10 food dishes that might be of interest to them whenever they log onto the application What's On The Menu (WOTM) specified in Section \ref{section:wotm}.  

Tapestry \cite{koren2009matrix,  goldberg1992using, itembased, survey} was one of the earliest recommender systems to introduce collaborative filtering in \citeyear{goldberg1992using}, created to handle electronic documents. It used manual collaborative filtering, allowing users to query information based on other's opinions about the documents. These opinions were in the form of annotations or replies made on documents, increasing probability of relevant results returned from queries \cite{schafer2007collaborative}. Tapestry relied on opinions from a small community such as an office workgroup where each person's opinion was trusted. Larger communities could not rely on every person knowing each other, leading to new collaborative filtering techniques being developed such as neighbourhood methods \cite{itembased}. 

Three main areas CF consist of memory-based CF techniques, model-based CF techniques, and hybrid-based CF techniques. Neighbourhood methods are a form of memory-based CF techniques, latent factor models are a form of model-based CF techniques, and hybrid-based collaborative filtering techniques are a combination of collaborative filtering techniques and other recommender techniques such as content-based filtering \cite{survey, koren2009matrix}.

\section{Neighbourhood Methods}

GroupLens \cite{grouplens} were the first to introduce a neighbourhood collaborative filtering technique. Building upon the Tapestry concept \cite{goldberg1992using}, GroupLens created an automated user based collaborative filtering technique for recommending Usenet articles that users may be interested in. 

Neighbourhood CF methods are classified as memory-based CF techniques because the entire ratings matrix is used to find similarity between users (or items) through a similarity measure \cite{memorybased, schafer2007collaborative}. 

Similarity measures used to find similar users (or items) are Pearson's Correlation \cite{zeng2003similarity, itembased, mahoutaction}, Cosine similarity \cite{itembased}, Euclidean distance \cite{similarity, mahoutaction}, and so fourth. These similarity measures are used in neighbourhood approaches such as K-Nearest Neighbour. Other neighbourhood algorithms are K-Means \cite{survey}, K-d Trees \cite{survey}, and Locality Sensitive Hashing \cite{survey}. 

Recommendations are given to users based on similarity of users (or items). This involves finding all user-user similarities (or item-item similarities) and using neighbourhood algorithms such as K-Nearest Neighbour to find the K most similar users (or items), referred to as neighbours. The K neighbours are then used to predict how the active user will feel about specific items, thus being able to make recommendations based on the neighbours. Since these neighbours may contain quirky characteristics that are not common among users, methods tend to take the weighted average of the neighbours ratings or simple weighted average to generate predictions for the active user \cite{survey}. 

The advantage of neighbourhood methods is that they are intuitive, easy to implement, and produce highly effective results \cite{survey, scalable,zeng2003similarity}. Memory-based methods such as K-Nearest Neighbour consist of real-time computation where recommendations are based on the most up-to-date information from users with relative good accuracy. However, since these methods perform computation on the whole dataset, they suffer from scalability problems with large datasets, additionally performance suffers from sparse datasets. Therefore, these methods may be impractical for real-environments where users and items are continually increasing \cite{zeng2003similarity}. 

\subsection{User-based Collaborative Filtering}

The defining characteristic of user-based CF is based upon similarities between users \cite{mahoutaction}. An example would be two users that rated the same food dishes similarly. The first user enjoys eating burgers indicated by positively rating various burgers. Similarly, the second user indicated from their ratings they enjoy burgers too. A similarity between the two is now inferred, based directly on their food preferences. Since they share common food preferences, recommendations can be given to each based on the assumption they will prefer new food preferences from each other. The logic of user-based recommendations is based upon this idea \cite{mahoutaction}, typically on a larger scale - more ratings, more users, and more items involved. 

User-based CF is typically used in neighbourhood techniques. Despite providing accurate recommendations, user-based CF techniques are computed in real time and their performance degrades as more users and items are added to the system, leading to scalability and performance issues \cite{dimension, itembased, evaluationitem}. This required collaborative filtering techniques that could easily scale and still produce high quality recommendations, leading to the exploration of item-based collaborative filtering.
 
\subsection{Item-based Collaborative Filtering}

Item-based collaborative filtering techniques were developed to address scalability limitations of the user-based techniques \cite{survey}. Item-based CF recommends new items to users based on item-item similarities. These recommendations are based on users previously preferred items \cite{mahoutaction}. Similar items are based on rating patterns of other users rather than attributes of the item; two items are considered similar if users rate them similarly \cite{schafer2007collaborative}.

Consider an example where a user has previously liked the following dishes: Thai Green Curry, Roti Channai With Chicken, and Butter Chicken Pie. Based on the users previously rated items, item-based CF finds similar items based on similar ratings from others. This is the key distinction between user-based CF and item-based CF, where items are recommended either based on similar users, or based on similar items \cite{mahoutaction}.    

\citeauthor{itembased} \cite{itembased} analyzed various item-based recommendation algorithms, computing item-item similarities and comparing the accuracy with traditional KNN user based collaborative filtering techniques \cite{itembased}. \citeauthor{itembased} found that items remained fairly static in the system, whereas user behaviours and preferences would often change. Because items were found mostly static, it meant precomputation could occur for item similarities. By having precomputed item similarities, traditional item-based collaborative filtering can then be applied, thus performance and scalability would be increased \cite{scalable}.
 
\section{Latent Factor Models}

Latent factor models can be classified as model-based CF techniques. They recommend items by automatically inferring meaningful information about users from previous rating patterns. For example, a model-based method can automatically attempt to learn about properties or characteristics of items, and learn the extent to which users prefer each of these properties, also known as latent factors \cite{koren2011}. This is used to make intelligent predictions based on the meaningful information inferred \cite{survey}. 

Model-based collaborative filtering techniques have been investigated to overcome the performance and scalability issues of previously mentioned CF techniques. Well-known model-based techniques include Bayesian belief nets \cite{baysian}, clustering models \cite{clustering}, and latent semantic models \cite{latent}. These models are based on learning patterns from users previous actions to predict new items and are expensive, but can be built offline allowing high scalability. The resulting model is ``very small, very fast, and essentially as accurate as nearest neighbor methods" \cite{itembased}. \citeauthor{itembased} found Bayesian networks to be practical in the context where user ``preferences change slowly with respect to the model" \cite{itembased}. However, these models are not suitable for environments where the user preference model should be updated rapidly or frequently. Since model-based approaches do not have to compute similarity measures to form neighbourhoods, they tend to produce faster recommendations and outperform neighbourhood models in terms of accuracy of recommendations \cite{toward, itembased}. 

In 2006, the Netflix Prize competition attracted interest in the field of recommender systems \cite{survey}. Netflix offered a \$ 1 million dollar prize to the first team that could improve their movie recommender system accuracy by 10\%. This attracted interest in the research field of recommender systems. The Netflix dataset \cite{winnings} consisted of \textit{177,770} movies, \textit{480,189} users, and a total of \textit{100,480,507} ratings collected over approximately 7 years \cite{winnings}. The team ``BellKor in Pragmatic Chaos" won the competition in 2009 basing their solution on a combination of latent factor models and neighbourhood models \cite{winning, survey}. These models took into account many biases which improved the predication accuracy. \citeauthor{koren2009matrix} \cite{koren2009matrix} were part of the winning team, and wrote a paper explaining how temporal effects, and user biases could be accounted for in latent factor models such as Singular Value Decomposition (SVD), making it superior to neighbourhood methods because of its flexibility \cite{koren2009matrix}. \citeauthor{koren2011} later published a paper about their findings and solutions which included a SVD++ \cite{winnings, netflix_course} approach to the Netflix Prize competition in \cite{koren2011}, \cite{winnings}, and \cite{winners}.

Matrix factorization is seen as one of the successful techniques of latent factor models \cite{memorybased, koren2009matrix}. Singular Value Decomposition is a matrix factorization technique that is well-known and can be applied to the CF domain, identifying latent semantic factors and becoming popular from combination of good scalability, predictive accuracy, and flexibility for modelling real-life situations \cite{koren2009matrix}. 

\subsection{Singular Value Decomposition (SVD) / Matrix Factorization}

Conventional Singular Value Decomposition (CSVD) requires factoring the user-item ratings matrix when applied in the CF domain. This introduces problems since CSVD is undefined when the user-item rating matrix is incomplete and fitting the known entries in the matrix leads to overfitting \cite{koren2009matrix}. To solve this, imputation can be performed by filling missing ratings in the user-item rating matrix. This can be very expensive, making conventional SVD computationally expensive \cite{koren2009matrix}. In the context of recommender systems, an approximate and iterative version of Singular Value Decomposition (SVD) \cite{winnings, netflix_course} was made popular by \citeauthor{simon_funk} \cite{simon_funk} during the Netflix Prize Competition \cite{netflix_course, simon_funk, koren2009matrix}. A matrix factorization \cite{koren2009matrix} approach was used to directly model observed ratings while using a regularized model to avoid overfitting \cite{koren2009matrix}. Throughout the Netflix Prize Competition, this matrix factorization approach was referred to as SVD \cite{netflix_course, winnings}, thus, we refer to Singular Value Decomposition (SVD) in this context. 

In the context of this project, the main goal of SVD is to train a model to learn what dish properties a user likes. This can be used to predict how a user feels about another dish from the properties inferred that the user will prefer. 

SVD decomposes the ratings matrix, where both users and items are mapped to a joint latent factor space of dimensionality \begin{math} f \end{math} - the space containing the inferred properties of food dishes, and how the users feel about those properties. The latent space tries to explain the ratings given from the ratings matrix by considering these inferred latent factors. These latent factors are represented in the form of an item vector \begin{math} q_{i} \in \mathbb{R}^f  \end{math} and a user vector \begin{math} p_{u} \in \mathbb{R}^f  \end{math} where \begin{math} i \end{math} is a given item and \begin{math} u \end{math} is a given user. The vectors contain a specified number of factors for each item and user, which are used to predict how the user may feel about other items they have not yet rated \cite{koren2009matrix}. Factors in the item vectors could be dimensions regarding properties in an item. An example for food dishes are item vectors that discover factors measuring obvious dimensions in the food dish such as ingredients, spice levels, cuisine type, meat type etc. Other dimensions may be discovered that are uninterpretable or less defined such as the amount of carbohydrates in the food dish \cite{koren2009matrix}. User vectors contain factors measuring the extent the user likes those factors which are properties inferred from the food dishes \cite{koren2009matrix}. These factors in \begin{math} q_{i} \end{math} and \begin{math} p_{u} \end{math} can be positive or negative, representing the extent of the properties contained within each dish, and how much the user likes those properties. 

These vectors contain a specified number of factors for each item and user, which are used to predict how the user may feel about other items that they have not yet rated \cite{koren2009matrix}. For this model, a user's predicted rating for a dish would be the dot product of the food dish vector, and the user vector \begin{math} q_{i}^T p_{u} \end{math}. This approximates to the active user's actual rating of the current item, denoted by \begin{math} r_{ui} \end{math}, leading to the estimate \cite{koren2009matrix}.

\begin{equation}\label{eq:1}\tag{1} \widehat{r} = q_{i}^T p_{u} \end{equation}

Since the user vector represents how much they like about the characteristics of the dish, and the item vector represents the portion of how much of the dish contains those properties \cite{koren2009matrix}, the dot product can be used to predict how the user feels about the dish - multiplying the properties within the item vector of the food dish with how much the user likes the corresponding properties from the user vector, repeating this for all elements in the vectors to see the overall interest the user has of the dish. This represents the active user's interest about the overall dish and is used to estimate the rating a user will give to the item. 

To infer the item and user vectors explained above, the system learns the model by fitting the previously observed ratings. However, directly modelling the observed ratings can lead to over fitting, being overspecialized leading to inaccurate future predictions. To prevent this, the idea is to use a regularized model that is used to generalize the previous ratings in a way that is able to predict future unknown ratings. To learn the vectors for the items and users, the system uses an equation that minimizes the regularized squared error on the set of known ratings.

\begin{equation}\label{eq:2}\tag{2}
\displaystyle min_{q*,p*} \sum_{ (u,i) \in K} (r_{ui} - q_{i}^T p_{u}) + \lambda (\| q_{i} \|^2 + \| p_{u} \|^2 )
\end{equation}

\begin{math} K \end{math} represents the training set from the rating matrix where \begin{math} r_{ui} \end{math} is known for the user, item pairs \begin{math} (u,i) \end{math}. The constant \begin{math}\lambda\end{math} controls the regularization used to penalize the magnitudes to avoid over fitting, and is typically tuned by cross-validation. Minimization can be performed by learning algorithms such as alternating least squares \cite{koren2011}.

\subsubsection{Alternating Least Squares (ALS)} \label{als}

The main purpose of Alternating Least Squares (ALS) is to find the minimum error from Equation \ref{eq:2} in SVD, which is used to learn the factors/features in each of the vectors based on the rating matrix. ALS works to find the item and user vectors by initializing the vectors with random values depending on how many factors are specified. From this, it fixes values one vector first (such as the item vector), then computes and reassigns values of the user vector according to the minimization equation, shown in Equation \ref{eq:2}. After this, it alternates - the user vector values are now fixed, and the re-computation of item feature values occur, trying to achieve the result of the smallest value for the minimization equation. This technique keeps alternating between fixing vector values, until there is a convergence where the values in the two vectors no longer change the minimization equation, or there are minimal changes. This is used to find the minimum error from Equation \ref{eq:2} to learn factors in each vector. 

\section{Hybrid-based Collaborative Filtering}

% \citeauthor{dimension} \cite{dimension} found a different approach that used dimension reduction techniques such as Singular Value Decomposition, making sparse rating models more dense by reducing the dimensionality of the product space, thereby condensing the modelled ratings of users and producing less missing information \cite{dimension}. 

Hybrid collaborative filtering techniques have been developed to utilise the advantages of content-based filtering (CBF) and collaborative filtering (CF) by combining multiple recommender algorithms \cite{toward, spiegel2010hybrid}. This attempts to alleviate the weaknesses and limitations of both techniques \cite{toward, spiegel2010hybrid}, but leads to increased complexity resulting in more expensive computations. Additionally, external information is needed about the content of the items which may not be available, thus making hybrid approaches impractical in certain scenarios \cite{survey}. 

\citeauthor{toward} \cite{toward} suggested creating user profiles consisting of demographic information which could be included in similarity measures. This would provide extra content to find similar users (or items) and effectively makes use of content-based filtering, learning attributes the user prefers \cite{toward}. Several hybrid collaborative filtering techniques can be found in \cite{toward} and \cite{spiegel2010hybrid}. To name a few, Weighted \cite{spiegel2010hybrid} hybrid CF techniques score items separately and are linearly combined from the results before they are shown to users. Switching \cite{spiegel2010hybrid} hybrid CF techniques involve multiple recommendation algorithms where differing algorithms may be used for various contexts, or when a particular algorithm can not produce recommendations to the users. An additional well-known hybrid technique includes content-boosted collaborative filtering \cite{hybrid}.

In this paper, a hybrid collaborative filtering system is implemented based on a combination of Weighted \cite{spiegel2010hybrid} and Switching \cite{spiegel2010hybrid} approaches. 

\section{Discussion}

Existing research focuses on the scalability and prediction-accuracy of the previous collaborative filtering techniques. Literature suggests that latent factor models such as SVD produce accurate recommendations and solve the scalability issue \cite{netflix_course, abergerrecommender, abergerrecommender}. Since SVD is a model-based CF technique, expensive computation can be performed offline, learning the model. Therefore, the resulting model's  performance is very fast online since precomputation has already been performed in the offline phase \cite{schafer2007collaborative}. This makes a model-based SVD approach the ideal candidate for the food recommendations application WOTM. 

Additionally, Alternating Least Squares (ALS) is selected as the learning algorithm in SVD. ALS is able to provide higher accuracy and better scaling on extremely sparse datasets \cite{abergerrecommender, koren2009matrix}. This enables utilisation of performance on implicit datasets, which may be useful for unary or binary ratings. Since WOTM has not been launched yet, the user-item rating matrix will be sparse, making ALS suitable for adapting to the extreme sparsity and also mitigating the ``Cold Start" problem. ALS can also be used for parallelization, since both vectors can be independently computed, decreasing the training rate and increasing scalability for future use \cite{koren2009matrix}. For these reasons, ALS is selected as the learning algorithm in SVD. 

It is evident there has been an abundance of existing research on collaborative filtering techniques and ways to improve the prediction accuracy. However, \citeauthor{schafer2007collaborative} \cite{interface} states these factors alone do not contribute to making a good recommender system \cite{schafer2007collaborative}. Instead, \citeauthor{schafer2007collaborative} \cite{interface} states that recommendation is not a ``one-size-fits-all problem"  \cite{schafer2007collaborative}. Specific tasks, information needs, and item domains represent unique problems for recommenders, and design and evaluation of recommender systems need to be done based on user tasks to be supported" \cite{schafer2007collaborative}. Similarly, \citeauthor{martin2009recsys} \cite{martin2009recsys} argues that the recommender algorithm is only one factor from many for providing recommendations to users. \citeauthor{martin2009recsys}  \cite{martin2009recsys} explains that the user experience, data collection, and other problems which make up the whole of the recommender experience need to be considered \cite{schafer2007collaborative, martin2009recsys}. \citeauthor{interface} \cite{interface} concludes that delivering accurate predictions to users in a way that creates the ``best experience for them remains an open problem" \cite{interface}. 

For this reason, this project focuses on the goal of providing a recommender system that fits the needs for the ``What's On The Menu" (WOTM) application in terms of the ``Find Good Items" use case. Specifically, it focuses on the recommendation system and any aspects involving the recommendation process. This involves aspects considering how the recommendation system will affect the user experience, the integration of a recommendation system within WOTM, and the underlying algorithms in the recommender system. 


