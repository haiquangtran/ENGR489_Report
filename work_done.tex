\chapter{Work Done}\label{C:work_done}

\section{Limitations}

The main limitations of this project is that it focuses on a mobile application, rather than a website application. This forces some limitations in terms of how users will interact with the system since with a mobile interface, the small form factor affects many changes which will be described later on. Since the existing components of WOTM are done in the programming language Ruby, it meant that we are restricted to this language. In addition, most of the data in the application was modelled, including the dishes, restaurants, users and so on. The only data that hadn't been modelled yet was the data required for the recommender system such as like/dislikes and preferences. The existing system used the database PostgreSQL which was another restriction.  

\section{Design Decisions}

\subsection{WOTM Recommendations Component}
TODO: Should this be in DESIGN DECISIONS???
PUT IMAGE.

WOTM Recommendations is made as an extension to the existing system. WOTM Recommendations is a REST API that is used to handle all the data needed to make recommendations to the users. It connects to the main WOTM API database (wotm\_dev) and stores previous user events such as the users likes/dislikes and their preferences. It uses this data to recommend food to the users and sends it back to WOTM Web API. 

The reason why we did not want to merge these events in the WOTM API is because it will then be tightly coupled to that system. By extracting it out, it means that WOTM API and WOTM Recommendations are not dependent on each other, and have their own sole purposes. WOTM API manages user and dish data, whilst WOTM recommendations manages everything to do with the recommendations. In that way, if we decide to remove recommendations, then we do not have to change code from the WOTM API, since it is loosely coupled to it. 

TODO: check if this is right.
Also by creating a new component it does not restrict us from using a different programming language if we desire. Since we are using REST, we are able to pass data from one component to another using a flexible language such as JSON or XML, however, we decided to use Ruby as the programming language as it meant that no extra overhead would be needed to convert data from one language to another. 

The advantage of using the programming language Ruby is that it is open source, and that the community for Ruby is large. Because of this, there will be libraries and ruby programs that are typically called 'gems' that other people have already created. 

\subsection{Open source projects}

Open source projects can be utilised to save time as we do not need to start from scratch and can adapt or modify projects to fit our specific needs. The open source community has a range of projects that are available and we are able to make modifications to suit our needs. In particular, a range of machine learning libraries are available that incorporate different types of collaborative filtering techniques. All that was needed was to set them up, and feed through our specific application data and run the algorithms that we needed. Spotify have an open source project called annoy \cite{annoy} that uses a nearest neighbours method of collaborative filtering written in C++/Python. Recommendable was another candidate that looked promising. Recommendable also uses neighbourhood collaborative filtering methods and is written in Ruby \cite{recommendable}. There were many other open-source software such as easyrec \cite{easyrec} and lenskit \cite{lenskit} that had a range of different collaborative filtering techniques. Eventually, we narrowed the list to what would appear to be the most suitable for this project below. 
\subsubsection{Apache Mahout}

Apache Mahout is an open source library written in Java \cite{mahout, mahoutaction}. Apache Mahout is made to run with Apache Hadoop to do parallel processing for scalability. Apache Mahout's main goal is to "build an environment for quickly creating scalable performant machine learning applications" \cite{mahout}. It contains many machine learning algorithms and forms of collaborative filtering. In particular, it contains user-based CF, item-based CF, matrix factorization models using alternating least squares, and a weighted matrix factorization approach using singular value decomposition. It also contains built in similarity measures such as Pearson's correlation, Euclidean distance, Cosine similarity, Spearman's Correlation and more. These similarity measures can easily be switched out with different CF algorithms and so on. 
Although Apache Mahout looks appealing, we did not choose to use it because it would require extensive overhead. Since it is a Java library it requires a Java Virtual Machine (JVM) to be executed. Since our database is in PostgreSQL we would need tools such as Java Database Connectivity (JDBC) to communicate with the database to feed the data to the algorithms. Another option would be using JRuby which is a implementation of Ruby that runs on a Java Virtual Machine allowing the use of Ruby and Java code for easy integration with Java libraries.
There is JRuby Mahout Library that could be used but is not maintained or updated frequently. Although a REST interface could be used with Java, it would require us to incorporate additional frameworks, in addition to using a server such as Tomcat to run on. Because of these reasons, we decided not to use this library for now.

\subsubsection{Recommendable}

Recommendable is an open source project written in Ruby that uses a user-based collaborative filtering approach. It allows you to add a recommendation engine for Likes and Dislikes into a Ruby application. Since our components are written in Ruby, it means that we can easily incorporate this gem into our recommendations component to provide recommendations. It uses a modified version of Jaccard's Index as the similarity measure to find similar users. Recommendable uses a key-value store called Redis which is used typically used as a cache system for other systems but in this case, it is used to deliver recommendations. The reason why Redis is suitable for collaborative filtering is because collaborative filtering logic "is based almost entirely on set math, and Redis is blazing fast for this" \cite{recommendable}. Recommendable only supports Binary events such as Like/Dislike, however it contains other events such as bookmarks which allows users to store their favourite items. 

We decided to implement and use this open source software first because it provides basic functionality for collaborative filtering that we can easily modify. In particular, we found it to the be easiest to integrate with our existing components to get recommendations working quickly because it is written in Ruby, and there is clear documentation. Because of this, we decided that we should get a basic form of collaborative filtering working first, rather than start off with anything complex. From this we will extend this system to suit our needs, being able to easily swap out the similarity measures for different ones. 

\subsubsection{PredictionIO}

PredictionIO is an open-source machine learning server to build and deploy prediction applications \cite{predictionio, predictionio2}. PredictionIO is built ontop of Spark, a faster alternative to Apache Hadoop, which utilizes parallel processing for large distributed datasets. Developers are able to build predictive engine components with separation-of-concerns because it utilises a Model View Controller architecture that achieves low coupling. These engines are deploy as web services having a REST interface, which allows our applications to query against the web service. By deploying the engines as a web service it means that these engines are not dependent on each other, nor are dependent on our application. For instance, we can easily swap and evaluate different collaborative filtering algorithms on different engines. We can deploy these engines as a web service, and all our 'WOTM' application has to do is query the engine to receive recommendations from the engine. Various algorithms can also be built on different engines and then the results can be combined to provide a way to implement hybrid collaborative filtering techniques. 

TODO: PUT IMAGE IN 

All we have to do is to send our data to PredictionIO's event server. PredictionIO's event server stores our data in a distributed database called HBase which provides high availability and is high scalability. Since it is distributed it gives us the option to do parallel processing in addition to being able to easily scale. Data from the event server is then used by the algorithms on the engines to train and learn the recommendations. Our application can then query this engine via REST methods to retrieve the recommendations. 

The reason why this is so appealing is because we can easily swap out different engines containing different collaborative filtering techniques. This saves times because we do not have to deal with wiring up REST calls or think about how we can connect different CF algorithms to our WOTM application. A lot of the work is done so that we can focus more on the algorithms part of it. Since the event server contains all of our WOTM information, we just deploy new engines with different CF algorithms on it and evaluate the results. Additionally, these engines implement a DASE architecture which provides separation and independence. DASE stands for Data, Algorithms, Serving, and Evaluator. These components are all considered to be loosely coupled to each other in the engine, meaning that we can implement different algorithms and at the serving phase, we can choose to combine the results from the different algorithms to produce a hybrid approach to recommendations. 
PredictionIO already contains a vast range of different engines that we can modify, including engines that incorporate latent factor models for Collaborative filtering. The community is active and new engines are constantly being produced. Additionally, there is abundant documentation as well as support on Google Groups and so on. 

For these reasons, we found PredictionIO to be our best candidate and decided to use it. 

\subsection{Online Learning vs Offline Learning}

With a mobile application, the speed, performance, and how smoothly the application runs is arguably more important the accuracy recommendations. For instance, a user is more likely to use a recommender application that less accurate recommendations but provides fast feedback and response time, as opposed to an application that provides accurate recommendations but is slow, and lags. In this case, the former is to be more accepted, because users will be able to easily skip inaccurate recommendations, whereas in the latter, users will have to wait for the recommendations to appear which may cause their attention to drift. 

With this in consideration, it is important to provide very fast recommendations, thus a model approach would be more appropriate than neighbourhood methods. A model collaborative filtering approach means that a model can be constructed and trained offline. Once the model is deployed, it will provide instant recommendations to the users because less computation needs to  be done since it has already learnt what the user likes offline. As well as this, a model based approach means that expensive computation can be computed offline, providing high scalability. However, the disadvantage of a CF model based approach is that it must be regularly trained to take into account the recent new actions of the users. Therefore, regular intervals to train the model need to be defined depending on how often users rate dishes.

Neighbourhood methods of collaborative filtering perform online learning, meaning that they do the computation in real time. For instance, in user-based CF when a user likes a dish, then the algorithm will be performed then and there, finding similar users, then finding neighbours that are similar to that user, providing a recommendation. This technique is easy to implement, but does not scale well, and is slower than the modelled approach. However, depending on the size of data that is expected to be collected from this 'WOTM', it may be a valid approach if the data is not expected to be large (millions of ratings). As new users and new items grow in the application, it is expected that the amount of ratings will exponentially grow as well, therefore there is a possibility that scalability may have to be considered in the long term. As of now, we will focus on the performance of the recommender system.  

For this reason, we will examine both online learning and offline learning, and see how implementations of the collaborative filtering algorithm affect the performance of the recommendations that are produced.  


\subsection{User Model}

With a mobile application it encourages interaction that is not present with websites. For instance, users are encouraged to use touch events to navigate across different pages and execute commands. This can make it difficult to collect certain events, and model these events in a database. The following section explains the design decisions of the events we choose to collect from users, implementing it in our WOTM model. 

\subsection{Explicit Feedback}

Recommender systems rely on different types of input data \cite{koren2009matrix}. Explicit data is referred to as a direct record of someones interest of an item. For example, Netflix collects star ratings for movies, and TiVo users collect data from having a thumbs up or thumbs down directly indicating that they specifically like or dislike a particular item \cite{koren2009matrix}. These events are mapped directly in the user-item matrix, and tell us directly how a user feels about an item. 
The next section explains the design decisions regarding the explicit feedback we will collect in the WOTM application.

\subsubsection{Boolean Ratings vs Ternary Ratings vs Likert Scale Ratings}

Although prediction accuracy is important, it is not the only factor that a recommender should focus on and acts as one facet in wide range of facets \cite{martin2009recsys}. \citeauthor{martin2009recsys} explains that the goal of a recommender system is to improve user experience however, designing a recommender system to fit a specific application remains a challenge, and that recommender system ratings should be highly based on the user experience \cite{martin2009recsys}. By focusing on the user experience, users will be able to easily rate food dishes they like, in turn, leading to the system collecting more recommendations from the ease of use. For this reason, the simplest model that allows users to easily rate a dish would be a simple boolean rating such as Like/Neutral or a ternary rating such as a like/neutral/dislike. The advantage is the ease of use for the simplicity, however it means that recommendations will not be as accurate as explicit rating values such as from 1-5 stars or 1-10 stars. By easing the user experience for rating food dishes it can increase data collection at the expense of accuracy. In fact, \citeauthor{movieratings} found in a study of user rating behaviour for movie tags, users that had the option to "like" or "dislike" provided more ratings than users with only one rating option \cite{schafer2007collaborative ,movieratings}. 

Using scaled ratings mean that we learn more about the user preferences because of the scaled factor indicating how much the user likes a dish or not. This means that model based collaborative filtering techniques are able to learn what the user likes and dislikes faster leading to more accurate recommendations. However, in terms of scaled ratings, it is difficult to see how good a dish is by using a likert scale system such as a 1-5 star rating system since the ratings will be skewed. For example, if a dish had a 5 star rating, it usually means that only 1 or 2 people have rated that dish. Ratings such as 3.5 stars may mean that it is a good dish, but from the way it is displayed, may seem like it is not as not as good as it looks. Good dishes may be perceived to be good dishes by users if they are are between 4-5 stars \cite{recommendable}. \citeauthor{interface} also explains that by displaying what other users think of the dish, the active user tends to rate accordingly to that of what other people think as well, leading to bias ratings \cite{interface}. An example would be a user rating a dish higher than they usually would because the food dish already has 4.5 stars. This can lead to inaccurate recommendations for the active user in the long term, because users may be influenced by others opinion \cite{interface}. \citeauthor{interface} argues that the way ratings are collected, and displayed influence how others will rate the dish. Considerations like this have to be taken into account to understand how the user will perceive and interact with the recommender system.  

A trade-off to consider is whether or not accurate recommendations are more important than the user experience. Since What's On The Menu aims at being a mobile application, the limitations are the small form factor that mobile phone screens have. With a scaled rating system, the user has to be shown these possible options in order to rate the dish, which may take up additional space that is not needed on such small screens. Because of this, having a 1-5 star rating system may degrade the user experience as opposed to a simple like/dislike rating system. This may lead to less data being collected and in the long term affect the recommendations that are provided to the users. But on the other hand, since it uses scaled ratings, it means we are able to learn more information from each rating. This means that the recommender system is more likely to predict accurate recommendations with less ratings. 

However, one could argue that the accuracy of the ratings may not matter a great deal. For instance, a recommender system could predict two dishes the user  may like based on previous rating patterns. The first dish is predicted with a 90\% prediction accuracy, and the second dish is predicted with a 70\% prediction accuracy that user will like these dishes. But is the difference in prediction accuracy important if the user likes both dishes? As long as the recommender system has provided the user with dishes they like, the interval of accuracy between those predicted dishes do not drastically matter. In addition to this, dishes with higher predicted accuracy may seem like obvious choices of dishes that users may have already tried, whereas dishes with a lower predicted accuracy may be less obvious dishes that they may like, but have not tried yet. It should not matter how much the user likes the dish as long as the user likes the dishes that are recommended. A recommender system using boolean or ternary values would eventually reach prediction accuracy similar to using likert scale ratings. Although this will happen in the long term, new ratings will most likely lead to less accurate predictions that join the system in the short term until more ratings are collected. 

Foursquare is an application that asks users to rate items according to a series of questions. These questions consist of "What do you like about this place?", "What is this place known for?" and so on. From these questions they are able infer a particular rating for the item, as well as collect data from the users to give more accurate recommendations. This rating system may risk users not rating the items because of the long list of questions it asks. Users may also have short attention spans and do not want to do such tasks. \cite{martin2009recsys} also says that part of the challenge is to design interfaces "that give users control over the recommendation process without overwhelming the user or rendering the tool too complicated for novice users." 

For these reasons, we found that simple like/dislike events would best suit the collection of data for the recommender system because of the simplified structure which increases the user experience. The recommender system can be extended to take in additional events that may portray additional information such as a "want" indicating that a user "wants" a dish but has not yet tried it before. However, caution must be taken as more events will affect the user experience of the application. 

\subsection{Implicit Feedback}

When explicit feedback is not available, implicit data can be used to infer preferences from users \cite{koren2009matrix}. Implicit feedback is referred to as an indirect reflection of someones interest in an item. Implicit feedback could be from observing user behaviour. This could include their click through data, their browsing history, the way they react to certain events, their search patterns, and so on \cite{koren2009matrix}. Implicit feedback can be collected to increase the accuracy of recommendations to the users by being combined with explicit feedback, or can fill in the user-item ratings matrix when explicit events are not available, alleviating the 'cold start' problem as it makes the matrix more dense. The next section explains design decisions regarding the implicit feedback we will collect in the WOTM application.

\subsubsection{Additional Events}

Netflix and other sites such as Amazon \cite{koren2009matrix, schafer2007collaborative} use implicit feedback such as views and purchase history in their recommender systems. These events indicate some form of interest in the user, however are less practical to apply for a mobile application. For example, on a website, many products are able to be shown to the user. When a user selects a product it will indicate some form of implicit feedback such as the user is interested in that product. With a mobile application in the context of WOTM, it is difficult to show a range of food dishes to users, because of the small screen size. This can make it difficult to collect additional implicit feedback. As well as this, purchase history and similar events are not practical because users are unlikely to indicate that they have purchased a dish after they've tried it. Explicit events such as like and dislike already infer that they have tried the dish, which make up for the purchase history. The implicit event of commenting on a dish may provide valuable information, perhaps commenting on a dish means that there is a strong interest or disinterest in a specific dish. However, this may impractical because we do not necessarily know if the comment is good or bad, without using extraction techniques.

A consideration to think about is that we are able to collect feedback on how the user feels about the recommendations produced by the recommender system by displaying explicit events. This can be used to gather more events, to make predictions more accurate. For instance, a user can indicate that they liked/disliked the recommendation that was shown to them, or they could skip it altogether meaning that they they do not have an opinion about it. Although the flaw in this method is that recommended dishes may be dishes that they user have not tried yet. Therefore, they do not will keep skipping through the results of the recommendations and provide no valid feedback to them. Although this may happen, an assumption is that users may like a dish that they have not yet tried, but are interested in. 

For these reasons, we do not use any implicit events in our recommendation system but this may change in future. 

\section{Implementation}

This section explains what has been implemented according to the design decisions in the previous section. 

\subsection{Neighbourhood Collaborative Filtering}

Using Recommendable, we have an implementation of the basic nearest neighbour collaborative filtering technique. It is able to provide recommendations based on like/dislike events of the users. In addition, we are able to implement bookmark events so users can save their favourite dishes. Recommendable also provides a method to see the most popular dishes, which can be shown to users if they have not rated any dishes yet. This approach uses online learning, and does real time computation. The recommender system uses user based CF with a modified Jaccard's similarity measure described below.

\subsubsection{Modified Jaccard Index}

Jaccardian similarity coefficient is an intuitive way of comparing people when the rating system is binary \cite{recommendable}. Recommendable uses a modified version. The modified version of Jaccard's Index is as shown:
TODO: EXPLAIN HERE
\[ u^n - u^n = z^n \]

\subsection{Latent Factor Models}

Since PredictionIO \cite{predictionio} is built ontop of Apache Spark, it provides many template engines that use a machine learning library called MLlib. Using PredictionIO, we have successfully built upon two engines that perform a model based CF approach: Matrix Factorization CF. This collaborative filtering technique uses offline learning. The two engines are described below and both use Alternating Least Squares to perform Matrix Factorization.  

\subsubsection{Similar Product Engine}

This template engine recommends products that are "similar" to items that a user has previously viewed. For example, item 1 and item 2 are considered to be similar if most users who view item 1, also view item 2 \cite{predictionio}. Therefore, similarity is defined by user's previous actions and not by the content of the items. 
By customizing this engine, we have changed the view event to be a "like" event instead. Therefore, this engine recommends to users similar items that they have previously liked, based on what others have liked. This is a form of item-based collaborative filtering but does not use any heuristics which does not make it a neighbourhood CF technique. Instead it uses matrix factorization which means it is classified as a latent factor model CF technique. These models need to be trained offline, but once trained and deployed, provide very fast recommendations to users. 

\subsubsection{E-Commerce Recommendation Engine}

This template engine is written in the programming language Scala, and was made to provide personalised recommendations for e-commerce applications with view events and purchase history events. It comes with the following out of the box functionality \cite{predictionio}.
\begin{enumerate}
 \item Exclude out-of-stock items
 \item Provide recommendation to new users who sign up after the model is trained
 \item Recommend unseen items only (configurable)
 \item Recommend popular items if no information about the user is available
\end{enumerate}

Since it uses implicit events, these events are recorded as binary events in the user-item rating matrix. An implicit event maps to a 1, and missing events map to a 0 in the matrix. This indicates some implication that the user is interested in the item. Additionally, implicit events also have a confidence value associated with it. These confidence values are formed by taking into account the preference values for each event and aggregating the identical events together. This increases the confidence values for multiple events that occur such as a user viewing the same item multiple times, meaning that there is a higher chance that products will be recommended based on this. 

Using this template, we modified this engine to take into account "Like" and "Dislike" events, removing the view and purchase events. We modified the CF algorithm in the engine to consider a "Like" event a positive result, giving it the preference value of a 1. In contrast, we made the algorithm int he engine consider a "Dislike" event to be a negative result, giving it the preference value of a -1. Since users may be able change their minds about liking or disliking a dish, we modified the collaborative filtering algorithm to only take into account the most recent like/dislike event that occurs from the user, if there are multiple identical like/dislike events on a dish from that user. I also added filtering in this recommendation system. Users are now able to filter recommendations based on their preferences such as their meat type, their cuisine type, and the food type. As well as this, users can see recommendations that are within their price range, if specified. 

If dishes are no longer available, then we are able to send a query to the recommendation engine to tell it that the dish is unavailable. Another feature is that we are also able to send a query to the recommendation engine to say that a user has already seen that dish, and not to recommend that seen dish anymore. In this way, the user only sees new dishes that they have not viewed yet.


This model recommends dishes to users based as soon as they have liked a dish. These recommendations are also based on the ratings of other users rating patterns, which means that the user-item rating matrix should be dense. If the model cannot learn what the user likes, or the user has not rated anything yet, then it defaults to recommending the most popular dishes to the users. The advantage of this is that users get to see the trending dishes that other users prefer, however the disadvantage is that it may create bias results in the system because popular dishes will only be seen by new users. This means that new users will only rate the popular dishes, causing problems in the recommender engine. To extend this engine, we should perhaps only recommend dishes to users after they have rated x amount of dishes. In addition, popular dishes useful to new users, so there should it should show random dishes to users. 
