\chapter{Evaluation}\label{C:evaluation}

\todo{Baseline Predictor?}
\section{Objectives}

The objectives of this report focuses on a base recommendation system that is able to fulfill the overall goal of ``Find Good Items" specified in section \ref{C:intro} \todo{Check this section is right}. To demonstrate whether the goal has been achieved, item popularity is used as a baseline predictor. Additional objectives focus on comparing variations of latent factor CF approaches to a hybrid CF approach presented in section \ref{algorithms} \todo{in regards to the overall goal of ``Find Good Items"}. The main objectives of the experiment are to answer the following questions:

\todo{CHECK THESE WITH SHARON, HOW DO WE KNOW IT IS A GOOD ITEM? NEED TO DEFINE IN INTRO. FILTER THESE OUT, SOME ARE VERY SIMILAR.}
\begin{itemize}
	\item{Can collaborative filtering provide personalised recommendations to users for the use case of ``Find Good Items"?}
	\item{Are personalised recommendations superior than recommendations based on item popularity for the use case of ``Find Good Items"?}
	% \item{What are suitable variations for binary events (Like/Dislike) for a ALS latent factor CF approach?}
	\item{Does a content-boosted hybrid CF approach provide superior recommendations to a standard latent factor approach?}
	\item{Do user preferences add value to collaborative filtering?}
	\item{What are suitable variations of a ALS CF approach that leverage binary events (Like/Dislike)?}
	\item{Do correlated binary events (Like/Dislike) provide more accurate recommendations than non-correlated binary events?}
	\item{What are suitable parameters for a latent factor approach deployable in a commercial environment?}
	\item{Is the ALS appraoch appriopriate for a real world commerical environment?}
	\item{Do ALS latent factor CF approaches ``Find Good Items"?}
	\todo{If have more time maybe try answer these}
	\item{\todo{Is item similarity CF more accurate than user based for latent factor models}}
	\item{\todo{Can the base recommendation provide recommendations during the Cold Start problem? or when the ratings matrix is sparse?}}
\end{itemize}

\section{Design}

\todo{The goal of the experiment is to provide a base recommendation system that provides personalised food dish recommendations to users in regard to the ``Find Good Items" use case.} CF heavily relies on user data to provide recommendations. To capture data, a survey was given to participants asking participants to rate their food preferences such as cuisine type, meat type etc. Additionally, the survey asked participants how they felt about food dishes. This data was used for an offline experiment to evaluate the latent factor CF and hybrid CF recommendation systems. \todo{Does this sentence make sense?} Quantitative results based on the survey data were used to evaluate the performance of the CF algorithms in regards to the goal of ``Find Good Items". The quantitative results consist of the following evaluation metrics: accuracy, precision, recall, area under the curve the receiving operator curve, and precision at top N (10). 


Due to time constraints and the amount of data required to evaluate CF approaches, an online/user study was unable to be conducted. Therefore, an offline evaluation is done to determine a suitable model for further online or user evaluation. 

\todo{The following sections further detail the design of the experiment, the reasoning behind the metrics used, and the results.}

\subsection{Setup/Tools}
The following equipment was used to run the experiment:
\begin{itemize}
	\item{Intel(R) Core(TM) i7-4770 CPU @ 3.40GHz}
	\item{NVIDIA Quadro K620 GPU 2GB DDR3}
	\item{8gb DD3 RAM}
	\item{Arch 64bit GNU/Linux Operating System}
\end{itemize}

\subsection{Participants}

A total of 91 participants were involved in the experiment. The majority of participants were students studying at the school of Engineering and Computer Science (ECS) from Victoria University (VIC). An estimated number of 55 students were from a first year ECS lecture and tutorial. An estimated number of 10 participants recruited were in their honours or masters year in ECS, and an estimated number of 20 participants were from a Summer of Tech event that was held at Kelburn Campus at VIC. The remaining participants were students from around the ECS labs from VIC.

The ages and gender of participants were not captured due to drawbacks in the ethics process. It is estimated the majority of participants are students in the age range of 17-24 since the majority were recruited from the School of Engineering and Computer Science at Victoria University. There may be a minority of participants that are under or over the age range of 17-24, and may not be students. In addition, it is likely there are more male participants than female participants due to the location of recruitment.

Participants were asked to fill out surveys capturing their food preferences, food intolerances, and their opinions about food dishes. The following details the data around the participants strict food preferences and collected from the survey:

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|} 
 \hline
 \multicolumn{2}{|c|}{Percentage of Participants with Strict Food Preferences} \\
    \hline
    \hline
    Participants & Percentage \\
     \hline\hline
     Vegetarian & 7\%\\ [0.5ex] 
     \hline
     Vegan & 5\% \\
     \hline
     Gluten Intolerant & 8\% \\
     \hline
     Nuts Intolerant & 0\% \\
     \hline
     Egg Intolerant & 0\% \\
     \hline
     Other Intolerance & 5\% \\
     \hline
\end{tabular}
\caption{Percentage of participants in the whole dataset that have strict food preferences.}
\label{table:food_participants}
\end{table}

\subsection{Survey}

Quantitative data was collected from participants in the form of a survey. Participants were asked to rate their food preferences, their food intolerances, and their opinion about food dishes from the following Likert scaled rating system: Hate, Dislike, Neutral, Like, and Love. Food preferences refer to preferences such as meat, pastry, soup, noodles etc. Food intolerances refer to intolerance to gluten, dairy, nuts etc. Food dishes refer to dishes such as Thai Green Chicken Curry, Fried Chicken, Butter Bean Salad and so fourth. Participants were asked to circle the rating that best reflected their opinion based on the Likert scaled ratings as seen in Figure \ref{fig:survey}. 

\begin{figure}
\centering
\includegraphics[scale=1.0]{images/survey_preferences.png}
\caption{Participants in the survey were asked to rate their food preferences and their opinions about food dishes based on the Likert scaled rating system that best reflected how they felt about the item shown. The figure above illustrates food preferences.}
\label{fig:survey}
\end{figure}

The survey contained 100 food dishes, 17 food preferences, and 5 food intolerances for participants to rate. Any unrated items were classified as Neutral ratings inferring participants had no opinion about the food dish at the current time of rating or that they had not tried the food dish before. The survey was grouped into categories for participants to filter through the food dishes efficiently. All participants were shown the same food dishes but the ordering of the categories were randomised to help mitigate biases such as participants rating only the first items they see and so fourth. All food dishes were captured from food vendors in the facility of Kelburn campus at Victoria University of Wellington. Only the title of the food dishes were shown to participants, other information such as images were explicitly excluded. This was to prevent biases around the imagery, the price of the dish, the food vendor of where the food can be purchased etc since these factors can influence the way participants rate the dishes (especially if the majority of participants are students). For example, a participant could like a food dish such as a Butter Chicken Pie from a specific food vendor, but hate it from another food vendor. The image of a food dish may also influence the way participants rate a dish, therefore, excluding this information enables the representation of the participants general food preferences to be captured. A variety of food dishes have also been included in the survey to ensure there are options for all users such as users with intolerances, and users that are vegans or vegetarians. The following section details the data collected from the survey. 

\subsection{Dataset} \label{dataset}

In the context of this project, a rating is referred to as a set of $(u, i, r)$ triplets, where a user $u$ has given a rating $r$ on an item $i$. 
The ratings collected from the survey consist of the following rating values: Hate (-1.0), Dislike (-0.5), Neutral (0.0), Like (0.5), and Love (1.0). Since Binary rating data is only used in the recommender system, Like and Love ratings are considered to be both positive ratings and can be considered as one event, in this case, a Like rating (Referring to any positive rating). Dislike and Hate ratings are also considered as one event and are combined to represent a Dislike event (Referring to any negative rating). Although this project focuses on Binary rating data only, the rationale of collecting multiple rating events was for backup purposes or future evaluation, if binary rating events are found to provide insufficient information to the recommender system. 

The original data collected from the survey consists of the following:
The dataset can be seen in the following table \ref{table:original_dataset}:

\todo{put into table}
\begin{itemize}
	\item{100 food dishes}
	\item{91 users}
	\item{9100 total ratings}
	\item{\todo{18223 Like/Love ratings}} 
	\item{\todo{1000 Dislike/Hate ratings}}
	\item{\todo{7000 Neutral ratings}}
	\item{673 food preferences}
\end{itemize}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|} 
 \hline
 \multicolumn{2}{|c|}{Original Dataset} \\
     \hline\hline
     Name & Number\\ [0.5ex] 
     \hline
     Users & 91 \\
     \hline
     Food Dishes & 100 \\
     \hline
     Available Food Preferences & \todo{17} \\ 
     \hline
     Available Food Intolerances & \todo{5} \\ 
     \hline
     Ratings & 9100 \\
     \hline
     Like Ratings (Positive) & \todo{545} \\
     \hline
     Dislike Ratings (Negative) & \todo{88} \\ [1ex] 
     \hline
     Love Ratings (Negative) & \todo{88} \\ [1ex] 
     \hline
     Hate Ratings (Negative) & \todo{88} \\ [1ex] 
     \hline
     Food Preference Ratings (Negative) & \todo{88} \\ [1ex] 
     \hline
     Food Intolerances (Negative) & \todo{88} \\ [1ex] 
     \hline
\end{tabular}
\caption{Table to test captions and labels}
\label{table:original_dataset}
\end{table}

\todo{range of food dishes, i.e. vegan, vegetarian, intolerance}
\todo{talk about SPARSITY \cite{zhang}}
\todo{Add average Like and dislike ratings per user.}
\todo{Add min and max ratings of likes and dislikes from user?}
\todo{Add min,max,and average number of ratings for each item}
\todo{Talk about density in regards to user and item orientation of ratings}
% 
% The sparsity of a rating dataset is defined as the density of the vacancies in
% the user-item rating matrix, as in equation 2.1.
% sparsity = 1 −
% # ratings
% # users × # items
% (2.1)
% Because most users would have only rated a very small portion of the
% items, most recommendation datasets have very high sparsity, with sparsity
% as high as 0.95 considered normal [129].3 For this reason, in the field of
% recommender systems, the term “sparse” only refers to the extreme cases
% where the sparsity of the dataset is higher than 0.99. Such extreme sparsity normally occurs when new recommender systems are first established, or
% in datasets where the number of users is small relative to the volume of
% information in the system due to either a large number of items or regular
% updates of the item pool or both.

\subsection{Data Cleansing}

Collaborative filtering algorithms depend heavily on user data to give personalised recommendations. Therefore, the validity of user data can influence how collaborative filtering algorithms perform on the data. To mitigate this, data cleansing is conducted, removing any information from users that provided dubious answers found in the survey. Many answers from the survey contained conflicting ratings from participants, examples were participants who indicated they were vegetarians, but had rated meat dishes positively. These users along with their rating information were removed and were apparent since authentic vegetarian participants rated ``Hate" for every food dish containing meat. A subset of users provided answers to the survey that were difficult to decipher, for example, participants indicated intolerances to gluten but had then positively rated food dishes containing gluten. This could mean several participants may have differing levels of tolerance to gluten, or perhaps expressed they enjoy the gluten free version of the food dish and so fourth. Therefore, any user data that was ambiguous was not removed from the dataset to maintain validity. In addition, cases where participants did not finish rating all items were preserved. Table \ref{table:cleansed_dataset} demonstrates the data after it had been cleansed:

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|} 
 \hline
 \multicolumn{2}{|c|}{Cleansed Dataset} \\
     \hline\hline
     Name & Number\\ [0.5ex] 
     \hline
     Users & 80 \\
     \hline
     Food Dishes & 100 \\
     \hline
     Available Food Preferences & \todo{17} \\ 
     \hline
     Available Food Intolerances & \todo{5} \\ 
     \hline
     Total Ratings & 8000 \\
     \hline
     Like Ratings (Positive) & \todo{3937} \\
     \hline
     Dislike Ratings (Negative) & \todo{1440} \\ [1ex] 
     \hline
     Neutral Ratings (Default) & \todo{2620} \\ [1ex] 
     \hline
     Total Food Preference Ratings & \todo{613} \\ [1ex] 
     \hline
\end{tabular}
\caption{Table to test captions and labels}
\label{table:cleansed_dataset}
\end{table}

\subsection{Overview}

In this experiment, an offline evaluation is performed to evaluate the performance of CF algorithms based on the goal of ``Find Good Items". The method used consists of partitioning the dataset, tuning parameters from the recommendation system, and evaluating the performance of CF algorithms using binary recommendation accuracy metrics \cite{zhang}. The following sections detail background knowledge required to understand the experiment, the experiment method, the experimental results, and the limitations and discussion. 

\subsubsection{Data Partitioning}

Standard offline evaluations consist of partitioning a dataset into a training set and a test set, containing rating triplets described in section \ref{dataset}. The training set is used by the recommendation system to learn the food preferences of users, providing recommendations based on predictions made about what the user from learning their preferences. Since the training set and test set are separate datasets, all rating triplets in the test set are unseen to the recommendation system and have not been learnt by the recommendation system during the training process. Therefore the ratings inside the test set is used to evaluate the performance of the recommendation system by checking whether the user has liked or disliked items recommended by the system. In addition, this is used to evaluate the generalisability of the recommendation system, giving insight to how well it may perform in a real scenario since the model has not seen the instances from the test data. In cases where the recommendation system contains tunable parameters, the dataset is partitioned into three datasets: a training set, a testing set, and a validation set. In the context of recommender systems, the validation set is used to find the optimal parameter settings but can also be used to reduce overfitting of the data, providing insight to the generalisability of the model. The validation set is used similarly to a test set to find the optimal parameters, after the recommender system has trained on the training set. After the optimal parameters have been found on the validation set, the test set is then used to evaluate the final performance of the recommender system. For partitioning the datasets, the \textit{skip-every-nth protocol} is used which is specifically tailored towards recommender systems \ref{zhang}.

The \textit{skip every nth protocol} \cite{zhang} is used to partition the dataset into a training set and a test set. The \textit{skip-every-nth protocol} consists of randomising the ordering of users, and randomising the ratings within each user in a sequence that is contiguous to the user, aggregating the result in a list sequence \cite{zhang}. The protocol iterates through this list of user ratings assigning every \textit{nth} rating to the test set, the remaining ratings are assigned to the training set. The \textit{skip-every-nth protocol} ensures that ``the sparsity of the training dataset is minimally disturbed, and guarantees a (n - 1):1 size ratio between training and test data for all users up to decimal rounding" \cite{zhang}. However, since the \textit{nth} value affects the user ratings that are involved in the test set, this means users with few ratings may not be in the test set since their ratings may be skipped during the partitioning process. In these cases, evaluation is done \textit{n} times with the same ordering, but offsetting the partitioning each run by 1. This ensures that every user rating is used exactly once in the test data \cite{zhang}. 

The experiments in this report use the \textit{skip-every-10th protocol} as the partitioning of the training set and the test set for final evaluation of the recommender system. This is detailed further in the report, in section \ref{method}.

\subsubsection{Parameter Tuning}

Each algorithm has several parameters which influences the way it performs on final evaluation, hence the need to find the optimal parameters. To find the optimal parameters, we tune the parameters on the training set created by the skip-every-10th protocol and use k-fold cross validation. In this way, the test dataset remains uncontaminated since the optimised parameters have not been trained on the test data. 
We use standard 10-fold cross validation to determine the optimal parameters. In the context of this project, the goal of k-fold cross validation is used to determine a model with optimised parameters during the training phase which will limit problems of over-fitting. 

\todo{May need to reexplain this. difficult to understand?}
K-fold cross validation consists of partitioning the dataset, in our case, the training set into k equal sized subsets. We randomise the training set before partitioning the set into k subsets. A subset from the k subsets is then selected as the validation data, and the remaining subsets (k-1) are used as the training data. Evaluation metrics are then calculated. This is repeated k times where each subset is used as the validation subset exactly once, and the results from the metrics are averaged to determine how good the parameters are for that run. This process is then repeated in multiple runs with different parameters, then the optimal parameters are chosen for the final evaluation. 
\todo{Sorry OK?}
\todo{
K-fold cross validation was chosen as it considers the variance in the data by averaging the results, and uses each data point exactly once, mitigating results the likelihood of lucky one-off results. In addition, it reduces the likelihood of overfitting. 
}
\todo{Add the main goal of K-fold which is to evaluate an overall system}

\subsubsection{\todo{Should I reiterate on the Recommendation Algorithms here?}}
\subsubsection{\todo{Baseline Predictor}}

\subsubsection{\todo{Recommendation} Accuracy}

In the context of recommender systems, recommendation accuracy measures the performance of a recommender system based on how well the system can correctly distinguish whether a user will like an item or whether they will not. This is similar to the decision end users make in a real world scenario where decisions are based on binary recommendations such as acceptance or rejection \cite{zhang}. Therefore, recommendation accuracy metrics are appropriate for the use case of ``Find Good Items" \cite{evaluation}. Therefore, we use recommendation accuracy as a metric since our goal is to ``Find Good Items".\todo{figure out where to put this} For recommendation algorithms that output rating predictions, a threshold-cutoff is used to determine the good items from the bad items \cite{zhang}.

\todo{This part is confusing}
However, there are caveats to using recommendation accuracy. Recommendation accuracy does not measure the quality of the items recommended as it does not measure the ability of an algorithm to accurately predict ratings, thus, does not focus on the correct ordering of the recommendations list based on the users preference of items. \todo{clarify this, may need to explain more such as deviations etc. Also top-N is not our use case... users could get bored after awhile. we don't care about ranking as long as users like the recs.}. This also means that it does not guarantee the user will see all the liked items in the top-N recommendations list. In addition, recommendation accuracy is identifying items that the user is already aware of, which makes it susceptible to biases involving overfitting and non-novel recommendations \cite{evaluation}. 

For these reasons, we use recommendation accuracy with supporting metrics of precision, recall, and precision at N. 

\subsubsection{Precision, Recall, and Precision at N}
\todo {Should I use all of these? Precision of the whole test data? Recall of the whole dataset? or just precision @ N?}

Binary recommendations can fall into one of four categories. \todo{TP, FP, TN, FN - Show table. Expand on this}

For binary ratings, precision and recall are often used as metrics. Precision measures the relevant recommendations (fidelity) to the user \todo{add equation for precision}, whereas Recall measures the relevant recommendations from the total preferred items (completeness) \todo{add equation for recall}. 

Precision at N looks at the top recommendations list, and sees how many true liked items are in that list, and how many false liked items are not in the list. This can be used to measure the portion of relevant items that are in the top-N recommendation list.

% Recall, in its purest sense, is almost always impractical to measure in a
% recommender system. In the pure sense, measuring recall requires knowing
% whether each item is relevant; for a movie recommender, this would involve
% asking many users to view all 5000 movies to measure how successfully we recommend
% each one to each user

% Perhaps a more appropriate way to approximate precision and recall would
% be to predict the top N items for which we have ratings. That is, we take a
% user’s ratings, split them into a training set and a test set, train the algorithm
% on the training set, then predict the top N items from that user’s test set. If we
% assume that the distribution of relevant items and nonrelevant items within
% the user’s test set is the same as the true distribution for the user across all
% items, then the precision and recall will be much closer approximations of the
% true precision and recall. This approach is taken in Basu et al. [1998].
% In information retrieval, precision and recall can be linked to probabilities
% that directly affect the user. If an algorithm has a measured precision of 70%,
% then the user can expect that, on average, 7 out of every 10 documents returned
% to the user will be relevant. Users can more intuitively comprehend the meaning
% of a 10\% difference in precision than they can a 0.5-point difference in mean
% absolute error

% One of the primary challenges to using precision and recall to compare different
% algorithms is that precision and recall must be considered together to
% evaluate completely the performance of an algorithm.

% Precision alone at a single search length or a single recall level can be appropriate
% if the user does not need a complete list of all potentially relevant
% items, such as in the Find Good Items task. If the task is to find all relevant
% items in an area, then recall becomes important as well. However, the search
% length at which precision is measured should be appropriate for the user task
% and content domain.

\subsubsection{ROC Curves}
\todo{redo}
In the context of this project, Receiver Operating Characteristic (ROC) curves \todo{CITE THIS} are used to visualise all the possible thresholds on a graph displaying how well a model is able to distinguish recommendations that the user will like or dislike. The True Positive (TP) rate which is the y-axis, shows the sensitivity of the model. This means that it shows the fraction in which liked items by specific users are correctly recommended to the users. The False Positive (FP) rate which is the x-axis, shows the fraction in which disliked items by specific users are incorrectly recommended as items they would like. 

\subsubsection{Area Under The Curve}
\todo{Explain this}


\subsection{Method} \label{Method}

\todo{Rewrite all of this. Just check if content is correct.}

The method focuses on two cases. The first focus is finding the optimal parameters for each algorithm to finalise the algorithm that performs best. This will be used for future prediction in an online-study or real scenario to see how real users will respond to each algorithm. The next focus is to compare which algorithm performs the best in an offline evaluation.

For the first method, we use the skip-every-10th protocol to obtain a training set and a test set. Using only the training set, we find the optimal parameters for an algorithm by performing 10-fold cross validation. Since cross-validation uses each subset as validation data, we retrain the model on the whole training set data with the optimal parameters obtained from the k-fold validation. Using this model, evaluation is done on the test set computing the recommendation accuracy, the precision, the recall, and the precision at N (10). The results are then averaged over 30 runs. 

\todo{Check this} Each different run uses the skip-every-10th protocol with an offset, so every test point is used exactly once. This method should give an optimal tuned parameters for future prediction used to finalize the system, however, contains biases in the test data. For these reasons, we do the second method to compare the algorithms, since the test set will not be contaminated with data from the tuning phase.

The second method is essentially the same as the first, except we tune the parameters each run to get the best results. In this way, the test data is uncontaminated and we can use this to compare the different algorithms when they perform at their best. However, this method means that the parameters are not finalized, hence why we do the first method. 

\todo{Which one is used in 4.3?}
% 1) tune the parameters using the whole data set and then do the normal leave-n out training and testing. (this is what Bing suggested) The parameters are tuned and they are finalized. But the testing has bias because some testing cases might be used in tunning. This method is good to finalize a system so it can be used for future prediction.

% 2) k-fold, for each round, you do training, tuning and testing, so you split the data into three parts. Using this method, the parameters are tuned for each round(you do the tuning for k times), so they are not finalized, so it is not good for predicting, but the testing has no bias.

\section{Experimental Results}

In this section we present the results of the experiments described in Section \ref{Method}. We will focus on comparing the hybrid CF approach to the standard ALS CF approach, demonstrating the use of food dish content used inside the hybrid CF technique is able to outperform the standard CF ALS approach. This section also demonstrates the optimal parameters for the different approaches for further investigation in a commercial/real world environment.  

\begin{figure}
\centering
\includegraphics[scale=0.7]{images/contaminated_accuracy.png}
\caption{Recommendation Accuracy from two variations of the ALS algorithm ran 30 times.}
\label{fig:algorithms}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.4]{images/results_tuning.png}
\caption{Averaged results ran 30 times with the same optimal parameters (contaminated).}
\label{fig:results}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.3]{images/single_als_30_runs.png}
\caption{Individual 30 runs of the standard single ALS algorithm. (contaminated)}
\label{fig:single_algorithm}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.3]{images/dual_als_30_runs.png}
\caption{Individual 30 runs of the dual ALS algorithm. (contaminated)}
\label{fig:dual_algorithm}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.3]{images/hybrid_als_30_runs.png}
\caption{Individual 30 runs of the Hybrid ALS algorithm which takes into account the content preferences of the user and considers these preferences when making recommendations. (contaminated)}
\label{fig:dual_algorithm}
\end{figure}

\todo{Check this - massive Brain dump to get content down (probably wrong as well)}
From running these algorithms multiple times, it is clear that the results give similar results. 

\subsection{Recommendation Accuracy}
Recommendation accuracy is the most suitable for the use case of ``Find Good Items". From the results, the hybrid collaborative filtering seems to provide the highest accuracy in this case. The preferences from each user describing the attributes the user likes and dislikes, provides extra information to the recommender system, thus resulting in the higher accuracy. However, this information may not be available. \todo{Hybrid achieved x accuracy}. The dual algorithm was able to provide more accurate predictions than the single algorithm. This validates objective \todo{do this} where the dual algorithm was able to correlate the events of likes and dislikes better than use of the single algorithm.

\subsection{Overall Precision}
\todo{Need to rewrite}
The precision is computed by iterating over the test set containing the rated items from the users, checking the whether the recommender system can accurately predict whether the test point is liked by the user, or disliked by the user. In terms of the goal of ``Find Good Items", precision looks at all of the recommended items from the recommender system, for completeness. From the results, Dual ALS was able to achieve a higher precision over the Single ALS approach over multiple runs (30). Dual ALS was able to achieve an average precision of 95.73\%, while Single ALS achieved an average precision of 87.38\%. This means the Dual ALS approach was able to more accurately recommend items the user preferred rather than the Single ALS. Since the average precision of the Single ALS is lower, it means that the recommender system recommended more items that users indicated they disliked (false positives).

\subsection{Recall}
\todo{Need to rewrite}
In the context of this project, Recall measures the fraction in which liked food dishes for each specific user is successfully recommended to the user. Essentially, it determines the sensitivity as it measures how many of the liked food dishes are recommended out of the total amount of actual liked food dishes by users. \todo{Fix this. Recall is closely related to Precision and their is usually a tradeoff between the two, since recall measures the amount of good items recommended, and precision measures the amount of disliked items that are recommended.} \todo{Need to add more}

\subsection{Precision at top N (10)}
\todo{Need to rewrite}
In addition to the overall precision, we decide to measure the results of the top 10 recommendations list to see whether the true liked dishes from the test set are within this recommendations list. Since it is unlikely that each user will have rated every item in the dataset, it means that the amount of false positives will be increased. The reason why we decided to see this, is because it is suited towards providing insight about how users may perceive the recommendation algorithm in a commercial environment. For example, in an offline evaluaiton, users have already indicated the food dishes they have liked. If these food dishes are displayed at the top of the recommendations list, in this case, we recommend the top 10 dishes, it may establish trust between the user and the recommender system. From the results, Dual ALS was able to achieve a higher precision within the top 10 recommendations list than the Single ALS approach.This means that more liked items from the test set appeared in the top 10 recommendations list when using the Dual ALS approach.    

% \subsection{\todo{Implicit events}}
% \todo{directly from design decisions}
% Seems like it can’t recognize negative values
% Using the implicit training is not able to classify negative values. 
% If you look at the values where the actual values were either hate or dislike, then all the predicted values are positive. This may be caused by the ratings of others on the dish.
% If we look at the data, It is skewed. There are far more likes/loves than dislikes/hates. This may cause the values in the item feature vectors and user feature vectors to all be positive, since all the positive ratings are overpowering the negative values. For instance, a user’s predicted score is reliant on the value of the item feature vector and the user’s feature vector of tastes. The cross product determines the user’s preference for the item. If many other users have rated the item being liked than disliked, then this will positively impact the feature values of the item feature vector such that the negative values are not even considered. 
% This may also affect the classification accuracy because all the predicted rating dishes are going to be based on positive ratings, meaning that the evaluation may be inaccurate. For instance, since all the item vectors are overpowered by positive ratings, then all the instances in the test set where the user has indicated that they “like/love” the dish will also be positive. This leads to biased results since every dish is positive within a threshold. 
% To resolve this, we may have to compute two engines, for likes and dislikes. 
% Also the cause of this may be because we are not training with explicit ratings.


\section{Limitations}
% \cite{martin2009recsys}
% Examples of this problem
% include the lack of standard treatment of items for which
% the recommender is unable to make a prediction. The broader
% goal of user-centered holistic evaluation, including A/B testing
% of the short- and long-term effects of recommendation differences,
% is still met by only a few research groups and companies
% that have the live systems and resources for such evaluation.

% Deploying innovative recommenders is still too hard, and there is a substantial need for research platforms where innovations can be tested without first building up a community of thousands of users. 
\todo{Write this whole section}
\todo{Rewrite all of these points into sentences}
\begin{itemize}
	\item{Offline experiment may not be enough, users may feel different}
	\item{Contradictions in the data and participation}
	\item{Proportion of items and user}
	\item{The labelling of data, removing the location, price etc}
	\item{The amount of users, and the amount of ratings, and the amount of items. Variety of food choices for vegans etc. }
	\item{The participants recruited - need to be more generalised (Mainly from students)}
	\item{Tuning the parameters? or unbiased datasets}
	\item{Online evaluation - novelty, serependity, etc, how can users give opinions about items they've never tried before}
	\item{Density and sparsity of the matrix}
	\item{The survey questions - need to ask the right questions and about how you ask the question.}
\end{itemize}
\todo{From design decisions}
Not enough representation of vegans or vegetarians
Not enough vegetarian or vegan dishes. 
This may cause recommendations to be minimal etc.
Also meat lovers may overpower the items?
Noticed that Vegetarians all rated items with meat as “HATE” this could affect the overall predictions for SVD?
May not be linearly separable?
People may not fill in survey correctly.
Some people may have filled out dishes that they haven't tried before. This could affect the results because often what people do is different to what they think. 
There were cases where people filled out the form inaccurately, such that they chose random dishes. We know this because there are cases where someone states that they are “vegetarians” but then say they like “meat” etc.
Cleanse these cases to give better predictions

\section{Discussion}

\subsection{\todo{Latent size}}
\todo{directly from design decisions}
\todo{Should I include stuff like this in the report?}
This affects the amount of features that will be used in SVD.

With the current amount of data, it seems that using a smaller latent size affects the accuracy. This is because having a smaller latent size will capture all the ``Strong" patterns from the ratings, as opposed to having a larger number of latent sizes, where the rating patterns are spread across more characteristics, meaning that the feature values for these characteristics will be smaller. More feature vectors -> the smaller the strength of the characteristics that it captures from the previous rating patterns. This may capture quirky characteristics from the users rating patterns...however, Less feature vectors -> the stronger the strength of the characteristics that it captures from the previous rating patterns. 

Overall, Dual ALS is performs better than single ALS in the context of using binary events.
% \section{\todo{Offline evaluation}}
% OFFLINE EVALUATION
% \todo{directly from design decisions}
% Offline evaluation does not determine whether users will like novel items or not. We cannot possibly measure this, since there are a vast amount of missing ratings from the users. 
% With an offline evaluation, we can only test against what the users have rated. Therefore, we cannot really test the novelty factor, as we can’t know if the user will like the top n list of recommendations since many will be missing. For this, we can use precision and recall but it means that the result will be dependent on the amount of rated positive items the user has done, and how many appear in the list. For this reason, there will be many FALSE POSITIVES, because users have not rated this item. 
% Therefore, an ONLINE evaluation would be the best to get a gauge on how the users really feel about the recommendations. There are a couple factors that are needed for this:
% A real system
% Users will have needed to rate a number of items
% There must be a vast amount of already rated items in the system
% Some users may not tried dishes so may not be able to label it accurately?




