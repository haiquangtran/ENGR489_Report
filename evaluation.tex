\chapter{Evaluation}\label{C:evaluation}

\section{Objectives}

In this experiment, we aim to compare a latent factor approach to collaborative filtering with a item-based collaborative filtering approach and a hybrid collaborative filtering approach. The main objectives are to answer the following questions:
\todo{All these questions need to change}
\begin{itemize}
	\item{Do dislike events add value to the results to a latent factor model CF approach}
	\item{What are the best variations of using like and dislike events for a latent factor CF approach}
	\item{Do using user preferences add value to collaborative filtering}
	\item{Is item similarity CF more accurate than user based for latent factor models}
	\item{Is hybrid CF more accurate than user based for latent factor models}
	\item{Does filtering recommendations by a black list more accurate for latent factor models}
	\item{Does filtering based on preferences of the user, more accurate for latent factor models}
\end{itemize}

\subsection{Threats to Validity}
\begin{itemize}
	\item{The survey questions - need to ask the right questions and about how you ask the question.}
	\item{Offline experiment may not be enough, users may feel different}
	\item{Contradictions in the data and participation}
	\item{Proportion of items and user}
	\item{The labelling of data, removing the location, price etc}
	\item{The amount of users, and the amount of ratings, and the amount of items. Variety of food choices for vegans etc. }
	\item{The participants recruited - need to be more generalised (Mainly from students)}
	\item{Tuning the parameters? or unbiased datasets}
	\item{Online evaluation - novelty, serependity, etc, how can users give opinions about items they've never tried before}
	\item{Density and sparsity of the matrix}
\end{itemize}

\section{Design}

\todo{talk about an overview here?}
The goal of the experiment is to provide personalised food dish recommendations to users in regard to the ``find good items" use case. A survey was given to participants capturing their food preferences and how they felt about specific food dishes. This data was used for an offline experiment to compare algorithms. For the use case ``find good items" we use several evaluation metrics: accuracy, precision, recall, and Top-N precision. These metrics are used in an offline experiment, however, an online experiment with the existing system should be conducted in future to validate the results. 
The following sections details the design of the experiment, the reasoning behind the metrics used, and the results. 

\todo{is it quantitative or qualitative data?}
\subsection{Setup/Tools}
\todo{Is this section even needed?}
The following equipment was used to run the experiment:
\begin{itemize}
	\item{Intel Core X}
\end{itemize}

\subsection{Participants}

A total of 91 participants were involved in our experiment. Most participants were students from the school of Engineering and Computer Science from Victoria University. \todo{60} students from a first year Computer Science lecture and tutorial at Victoria University were involved in the experiment by filling out surveys capturing their food preferences and their opinions about food dishes. \todo{10} honours and masters students from Victoria University were asked to fill out our surveys. \todo{20} participants were from a Summer of Tech event that was held at Victoria University. The remaining participants were students from around the Computer Science labs at Victoria University.

The ages and gender were not captured of the participants due to drawbacks in the ethics process. It is estimated that most participants are students in the age range of 17-24 since most were recruited from the School of Engineering and Computer Science at Victoria University. There may be a minority of students that are over that age range. In addition, it is likely there are more male participants than female participants due to the location of recruitment. Participants were asked to fill out a survey.

\todo{vegetarian users, intolerant users, gluten free users, vegan users etc}

\subsection{Survey}

Qualitative data was collected from participants in the form of a survey. Participants were asked to rate their food preferences, their food intolerances, and their opinion about food dishes from the following likert scaled rating system: Hate, Dislike, Neutral, Like, and Love. Participants were asked to answer the Likert scaled ratings by circling the rating that best reflected their opinion of the food dish (See Figure \todo{figure number}). The survey contained 100 food dishes, \todo{10} food preferences, and \todo{intolerances} for participants to rate. Any unrated items were classified as Neutral ratings inferring participants had no opinion about the food dish or that they had not tried the food dish before. The survey was grouped into categories for participants to filter through the food dishes efficiently. All participants were shown the same food dishes but the ordering of the categories were randomised to help mitigate biases such as participants rating only the first items they see and so forth. All food dishes were captured from food vendors in the facility of Kelburn campus from Victoria University. Only the title of the food dishes were shown to participants, other information was excluded. This was to prevent biases around price, food vendor of where the food is given, and so on, which can influence the way participants rate the dishes. For example, a participant could like a food dish such as a chicken burger from one place, but hate a burger from another place. Other factors such as price affect the participants the way they rate the dish. Therefore, excluding this information allows us to capture the participants preferences of food in general, rather than specific dishes. \todo{Redo} (Doing this would be difficult as well and participants would get bored of the survey etc)

\todo{Show image}
No images were used to bias the results of the study. 
\todo{range of food dishes, i.e. vegan, vegetarian, intolerance}
\todo{portion of vegetarians etc}
A range of food was shown to participants ranging from gluten free, vegetarian, and vegan dishes. \todo{Fix this}

\subsection{Dataset}
In this section, we refer to a rating as a set of (u, i, r) triplets, where a user u has rated r on an item i. 
\todo{Should this go in dataset or survey section?}
The ratings collected from the survey consist of the following rating values: Hate (-1.0), Dislike (-0.5), Neutral (0.0), Like (0.5), Love (1.0). Since we are using Binary data only in the recommender system, we consider a like/love rating as a like event, and a dislike/hate rating as a dislike event in the algorithms. The reason why the granularity of like/love, and dislike/love were included in our survey in case we changed our minds on the types of events to use in the system.

The original data collected from the survey consists of the following:
\todo{put into table}
\begin{itemize}
	\item{100 food dishes}
	\item{91 users}
	\item{9100 total ratings}
	\item{\todo{18223 Like/Love ratings}} 
	\item{\todo{1000 Dislike/Hate ratings}}
	\item{\todo{7000 Neutral ratings}}
	\item{673 food preferences}
\end{itemize}

\todo{talk about SPARSITY \cite{zhang}}
% 
% The sparsity of a rating dataset is defined as the density of the vacancies in
% the user-item rating matrix, as in equation 2.1.
% sparsity = 1 −
% # ratings
% # users × # items
% (2.1)
% Because most users would have only rated a very small portion of the
% items, most recommendation datasets have very high sparsity, with sparsity
% as high as 0.95 considered normal [129].3 For this reason, in the field of
% recommender systems, the term “sparse” only refers to the extreme cases
% where the sparsity of the dataset is higher than 0.99. Such extreme sparsity normally occurs when new recommender systems are first established, or
% in datasets where the number of users is small relative to the volume of
% information in the system due to either a large number of items or regular
% updates of the item pool or both.

\todo{Add average Like and dislike ratings per user.}
\todo{Add min and max ratings of likes and dislikes from user?}
\todo{Add min,max,and average number of ratings for each item}
\todo{Talk about density in regards to user and item orientation of ratings}

\subsection{Data Cleansing}

The rating data of Like and Dislike events need to be fed through the algorithms to give personalised recommendations to users. The legitimacy of rating data provided will affect the way the collaborative filtering algorithms perform since it makes recommendations based on similar users. In order to mitigate this, we cleanse the data by removing participants that gave contradictory answers before we import the data in the recommender algorithms.

%  There were participants that had indicated they were vegetarians, but then had also indicated they had a preference for meat, rating meat dishes highly. This affects the recommendations that are shown by the algorithms.  

By examining the data collected from the surveys, it was evident that not all participants answered the questions legitimately. The most obvious were participants claiming to be vegetarian but had indicated they liked food dishes containing meat. Legitimate vegetarian users were easy to distinguish as they rated ``Hate" for every meat dish. There were other cases that were difficult to classify as legitimate. For example, a few participants specified they had intolerances to gluten, but indicated they liked eating food dishes with bread. This could mean several things such as participants having differing levels of tolerance to gluten, or perhaps the participants meant they liked eating the food dish with gluten free bread. Ambiguity in these cases meant we did not remove any participants we were unsure about. In addition, there were also cases where participants did not finish rating all items, and also participants that skipped pages in the survey. We included these users in the ratings for completeness as it enables us to see how the recommendation handles sparsity of ratings from these users. 

Overall, we removed users having contradictory answers refining the user data to the following dataset:
\todo{put into table}
\begin{itemize}
	\item{80 users}
	\item{8000 total ratings}
	\item{3937 Like/Love ratings}
	\item{1440 Dislike/Hate ratings}
    \item{2620 Neutral ratings}
	\item{613 food preferences}
\end{itemize}

\subsection{Overview}

To compare each algorithm to see which performs the best for our overall goal which is ``Find good items", we run an offline experiment. The method used in this experiment consists of partitioning the dataset, tuning the parameters, and validating the model using performance accuracy metrics. The following sections explain the details needed to understand the method in our experiment. We then state the method we used for comparing the different algorithms. 

\subsubsection{Data Partitioning}

To evaluate the recommender system algorithms in a standard offline evaluation, we partition the ratings dataset into a training set and a test set. The training set is used by the recommender algorithm to learn the model, and the test set is used to test the learned model to see how it performs on unseen data. In this case, the algorithms learn a model of user food dish preferences from the training set data, and use this to predict food dishes the users will like to which we then check against the test data. This is used to validate the generalisability of the model and gives an indication of how well it may perform in a real scenario, since the model has not seen the instances from the test data.

The skip-every-nth protocol \cite{zhang} is used to partition the dataset into a training set and a test set. The skip-every-nth protocol consists of randomising the ordering of users, and randomising the ratings within each user in a sequence that is contiguous, aggregating the result in a list sequence \cite{zhang}. The protocol iterates through this list of user ratings assigning every nth rating to the test set, the remaining ratings assigned to the training set. The skip-every-nth protocol ensures that ``the sparsity of the training dataset is minimally disturbed, and guarantees a (n - 1):1 size ratio between training and test data for all users up to decimal rounding" \cite{zhang}. However, since the nth value affects the user ratings that are involved in the test set, this means users with few ratings may not be in the test set since their ratings may be skipped during the process. 
\todo{Ask Sharon about this} In these cases, we evaluate the recommender algorithm n times with the same ordering, but offsetting the partitioning each run by 1. This ensures that every user rating is used exactly once in the test data \cite{zhang}. 

\todo{Should this be here?}
In our experiments, we use the skip-every-10th protocol as our partitioning technique for the final evaluation of the algorithm performance. We run the evaluation metrics multiple times with the optimal parameters and average the results. 

\subsubsection{Parameter Tuning}

Each algorithm has several parameters which influences the way it performs on final evaluation, hence the need to find the optimal parameters. To find the optimal parameters, we tune the parameters on the training set created by the skip-every-10th protocol and use k-fold cross validation. In this way, the test dataset remains uncontaminated since the optimised parameters have not been trained on the test data. \todo{Redo this}
In this context, the goal of k-fold cross validation is used to determine a model with optimised parameters during the training phase which will limit problems of over-fitting. 

\todo{Reexplain this. difficult to understand}
The technique used to find the optimal parameters is k-fold cross validation. K-fold cross validation consists of partitioning the dataset, in our case, the training set into k equal sized subsets. We randomise the training set before partitioning the set into k subsets. A subset from the k subsets is then selected as the validation data, and the remaining subsets (k-1) are used as the training data. Evaluation metrics are then calculated. This is repeated k times where each subset is used as the validation subset exactly once, and the results from the metrics are averaged to determine how good the parameters are for that run. This process is then repeated multiple times with different parameters, the optimal parameters being chosen for the final evaluation. K-fold cross validation was chosen as it considers the variance in the data by averaging the results, and uses each data point exactly once, mitigating results the likelihood of lucky one-off results.

\subsection{Method}

\section{Results}


\subsection{Limitations}
% \cite{martin2009recsys}
% Examples of this problem
% include the lack of standard treatment of items for which
% the recommender is unable to make a prediction. The broader
% goal of user-centered holistic evaluation, including A/B testing
% of the short- and long-term effects of recommendation differences,
% is still met by only a few research groups and companies
% that have the live systems and resources for such evaluation.

% Deploying innovative recommenders is still too hard, and there is a substantial need for research platforms where innovations can be tested without first building up a community of thousands of users. 
\todo{Show proportions of data}


\section{Offline Evaluation}

\subsection{Data Partitioning}

\subsection{Parameter Tuning}

\subsection{Offline Metrics}

\subsubsection{Binary List}

\subsubsection{Classification Accuracy}
\todo{do this}
% Classification Accuracy Metrics. Classification metrics measure the
% frequency with which a recommender system makes correct or incorrect decisions
% about whether an item is good. Classification metrics are thus appropriate
% for tasks such as Find Good Items when users have true binary preferences.
% When applied to nonsynthesized data in offline experiments, classification
% accuracy metrics may be challenged by data sparsity. The problem occurs when
% the collaborative filtering system being evaluated is generating a list of top
% recommended items. When the quality of the list is evaluated, recommendations
% may be encountered that have not been rated. How those items are treated in
% the evaluation can lead to certain biases.
% One approach to evaluation using sparse data sets is to ignore recommendations
% for items for which there are no ratings. The recommendation list is first
% processed to remove all unrated items. The recommendation task has been altered
% to “predict the top recommended items that have been rated.” In tasks
% where the user only observes the top few recommendations, this could lead to
% inaccurate evaluations of recommendation systems with respect to the user’s task. The problem is that the quality of the items that the user would actually
% see may never be measured.

% In essence, we are measuring how well the system
% can identify items that the user was already aware of. This evaluation approach
% may result in collaborative filtering algorithms that are biased towards obvious,
% nonnovel recommendations or perhaps algorithms that are over fitted—fitting
% the known data perfectly, but new data poorly.

% Classification accuracy metrics do not attempt to directly measure the ability
% of an algorithm to accurately predict ratings. Deviations from actual ratings
% are tolerated, as long as they do not lead to classification errors. The particular
% metrics that we discuss are Precision and Recall and related metrics and ROC.

\subsubsection{Precision \& Recall}
\todo {precision and recall}

% Recall, in its purest sense, is almost always impractical to measure in a
% recommender system. In the pure sense, measuring recall requires knowing
% whether each item is relevant; for a movie recommender, this would involve
% asking many users to view all 5000 movies to measure how successfully we recommend
% each one to each user

% Perhaps a more appropriate way to approximate precision and recall would
% be to predict the top N items for which we have ratings. That is, we take a
% user’s ratings, split them into a training set and a test set, train the algorithm
% on the training set, then predict the top N items from that user’s test set. If we
% assume that the distribution of relevant items and nonrelevant items within
% the user’s test set is the same as the true distribution for the user across all
% items, then the precision and recall will be much closer approximations of the
% true precision and recall. This approach is taken in Basu et al. [1998].
% In information retrieval, precision and recall can be linked to probabilities
% that directly affect the user. If an algorithm has a measured precision of 70%,
% then the user can expect that, on average, 7 out of every 10 documents returned
% to the user will be relevant. Users can more intuitively comprehend the meaning
% of a 10\% difference in precision than they can a 0.5-point difference in mean
% absolute error

% One of the primary challenges to using precision and recall to compare different
% algorithms is that precision and recall must be considered together to
% evaluate completely the performance of an algorithm.

% Precision alone at a single search length or a single recall level can be appropriate
% if the user does not need a complete list of all potentially relevant
% items, such as in the Find Good Items task. If the task is to find all relevant
% items in an area, then recall becomes important as well. However, the search
% length at which precision is measured should be appropriate for the user task
% and content domain.


\subsubsection{ROC Curves}

\subsubsection{Thresholding}

\section{Online Evaluation or User Study}

\section{Results}

\section{Limitations}

\section{Discussion}

