\chapter{Background Survey \& Related Work}\label{C:background}

\section{Collaborative Filtering}

Collaborative filtering (CF) is a popular technique that has been successfully used in recommender systems \cite{itembased, schafer2007collaborative, survey}. The main idea behind collaborative filtering is that rating behaviour of others can be used to predict items that the active user will be interested in. Intuitively, this algorithm stems from the assumption that users that have similar opinions for items that they have both rated, tend to have similar tastes to each other, usually having common opinions about other items  \cite{schafer2007collaborative}. For example, most of us have friends with similar opinions and tastes to us. Perhaps you and a friend both like eating chicken sandwiches. Since you and your friend have similar tastes, your friend may then be able to suggest to you another food dish that you might enjoy such as duck with rice. Collaborative filtering uses the same concept, but usually on a larger scale. 

TODO: Add matrix image. See figure 1.

Users express their preferences for a variety of different items through "ratings" which can be in different forms such as 1-5 star ratings, or ternary scale ratings such as likes/dislikes. These ratings are typically represented as a (User, Item, Rating) triplet. These ratings can be represented in the form of a user-item ratings matrix, where the rows of the matrix are the users, the columns of the matrix are the items, and the ratings are represented inside the matrix. Since the matrix contains sets of rating triplets, (User, Item) pairs will exist where a user has not yet rated the item, thus the matrix usually contains sparse values within it. 

Since the user-item ratings matrix can be sparse, collaborative filtering can struggle to address new products and users because there are not enough ratings or information to provide accurate recommendations to these users. Sparse ratings can also lead to another issue called the 'Cold Start' problem, where ratings for these new items or new users have not been collected yet.


(Content based filtering is superior in this context). In contrast, the advantage of collaborative filtering is that it does not require domain knowledge of the items or users to provide recommendations. It purely focuses on the previous behaviours of users and their rating history, providing recommendations that are generally more accurate than content based filtering \cite{koren2009matrix, schafer2007collaborative}. 

TODO: Take this out or leave in?

Collaborative filtering is preferred over content based filtering, a technique where items are recommended based on learning users preferences from previous rating patterns of the user, and recommending similar items based on the attributes of those items. Additionally, content based filtering is prone to recommending only a small subset of items, since it recommends items based on attributes of the items, leading to overspecialization thus restricting variety \cite{toward}. Another problem of content-based filtering is that it is difficult to extract features from the items and users, and manually labelling these features may not be practical \cite{toward}. Traditional collaborative filtering are able to address these concerns since attributes are ignored, and recommendations are based on actions of other users, providing a range of recommendations that are not restricted to specific attributes \cite{koren2009matrix}. For these reasons, we chose to not address content-based filtering, but explain it because hybrid collaborative filtering approaches use it. 

Using the user-item ratings matrix, a list of top N recommendations is usually given to the user representing the items that they will like, computed through collaborative filtering techniques. For example, the active user may be recommended 10 food dishes that might be of interest to them whenever they log on.  

There are two main areas that encompass collaborative filtering, these areas are neighborhood methods and latent factor models which will be explained in following sections \cite{koren2009matrix}. 

% \section{Challenges}
% \subsection{Cold Start}
% \subsection{Data Sparsity}
% \subsection{Synonyms}
% \subsection{Scalability}
% \subsection{Grey Sheep}
% \subsection{Shilling attacks}
% \subsection{Diversity and the Long Tail}

\section{Neighbourhood Methods}

Neighbourhood CF methods can be classified into the class of memory-based collaborative filtering because they use heuristics to find the similarity between items or users \cite{schafer2007collaborative}. The main idea of this approach is to use the user-item ratings matrix to compute the similarity between users or items which are then used for making recommendations. This typically involves finding all user-user similarities or item-item similarities, and then using neighbourhood algorithms such as K-Nearest Neighbour to find the K most similar items or users, usually referred to as the neighbours. The K most similar items or K most similar users are then used to predict how the active user will feel about specific items, thus being able to make recommendations for the active user based on their neighbours. Since these neighbours may contain quirky characteristics that are not common among other items or users. For this reason, methods usually tend to take the weighted average of the neighbours ratings or simple weighted average to generate predictions for the active user \cite{survey}. 

Common similarity measures that are used are Pearson's Correlation, Cosine similarity, Euclidean distance and so forth, finding similar users to the active user or similar items that the active user has previously performed actions on. The most common neighbourhood method that is used to find the most similar users or items is the K-Nearest Neighbour algorithm. This algorithm finds the K-nearest neighbours based on a heuristic, in this case, the similarity measure. Other neighbourhood algorithms are K-Means, K-d Trees, and Locality Sensitive Hashing. 

% A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

The following section explains the difference between user-based CF and item-based CF.

% +Advantages
%     -   explain
%     - easy to create etc
%     - easy facilitation of new data
%     - content independence of itemsbeing reocmmended
%     - good scaling with co-rated items
% +Disadvantages
%     - performance decrease with sparse data
%     - scalability and problems with large datasets
%     - adding new items required inclusion of the new item and re-insertion of all elements in the structure



\subsection{User-Based Collaborative Filtering}

User based CF uses a similarity measure to find like-minded individuals that are similar to each other, producing recommendations to the active user based on ratings of similar users. The defining characteristic of user-based CF is based upon the similarities between users \cite{mahoutaction}. A concrete example would be if two users that both like eating fast food. the first user has indicated that they like eating a Big Mac and also like eating the Whopper Burger. Similarly, the second user has indicated that they like eating the Big Mac. Therefore, since these two users previous behaviours are similar as they both have indicated their enjoyment of Fast foods, in this case, Big Macs, they are to be considered similar. We can then recommend to the second user that they should try the Whopper Burger, since the first user liked it and the two users have similar tastes. User-based CF has the same premise but typically on a larger scale - more ratings, users, and items are involved. 


\subsection{Item-Based Collaborative Filtering}

Another approach that is commonly used is called item based collaborative filtering. Instead of finding users that are similar to one another, item based collaborative filtering focuses on previous behaviour of the active user and recommends items that are similar to these items \cite{mahoutaction}. Similar items are extracted by the rating patterns of other users rather than the attributes of the item - Two items are considered similar if users rate them similarly \cite{schafer2007collaborative}. For example, a user could have previously liked the following dishes: a chicken sandwich, chicken nuggets, and chicken soup. Item based collaborative filtering will find similar items that the user has previously rated by using a similarity measure on their previous items they indicated they liked. By using this technique, it may recommend new items to the user such as chicken salad, or a chicken burger since all those items contain chicken. This is the main idea behind item based collaborative filtering. 

\subsection{Similarity Measures}

\subsubsection{Pearson's Coefficient}

This method computes the statistical correlation
(Pearson’s r) between two user’s common ratings to determine
their similarity.

\subsubsection{Euclidean Distance \& Manhattan Distance}
\subsubsection{Cosine Similarity}
\subsubsection{Jaccard Index}

\section{Latent Factor Methods}

Latent factor models look at analysing the previous actions of users and creating a feature vectors for items and users. These item and user features are used to predict future items that the user may like. 

The design and development of models (such as machine learning, data mining algorithms) can allow the system to learn to recognize complex patterns based on the training data, and then make intelligent predictions for the collaborative filtering tasks for test data or real-world data, based on the learned models. Model-based CF algorithms, such as Bayesian models, clustering models, and dependency networks, have been investigated to solve the shortcomings of memory-based CF algorithms [9, 71]. Usually, classification algorithms can be used as CF models if the user ratings are categorical, and regression models and SVD methods and be used for numerical ratings.

\subsection{Singular Value Decomposition}
\subsubsection{Stochastic Gradient Descent}
\subsubsection{Alternating Least Squares}


\section{Literature Review}

The term 'collaborative filtering' was first introduced in \citeyear{goldberg1992using} by \citeauthor{goldberg1992using} to describe the technique used in Tapestry, one of the earliest known recommender systems \cite{koren2009matrix,  goldberg1992using, itembased, survey}.

Tapestry \cite{goldberg1992using} was created to handle electronic documents and used manual collaborative filtering, allowing users to query information based on others opinions about the documents. These opinions were in the form of annotations or replies which users were encouraged to make on documents to increase probability of relevant results returned from queries \cite{schafer2007collaborative}. Tapestry relied on opinions from a small community such as an office workgroup, where each person's opinion was trusted. Larger communities could not rely on every person knowing each other, leading to new collaborative filtering techniques being developed \cite{itembased}. 

More recommender systems emerged as value was seen in the potential to increase sales from recommendations - customers may purchase suggested items that they might not have seen otherwise \cite{schafer2007collaborative}. Perhaps the most popular recommender system in the late 1990's was used in Amazon.com, collecting user purchase history, browsing history, and recently viewed items to recommend items that the user may buy \cite{schafer2007collaborative}. Other recommender systems consisted of Jester \cite{goldberg} for jokes, and Ringo \cite{ringo} for music.
% /cite{schafer2007collaborative, toward}
GroupLens \cite{grouplens} were the first to introduce a neighbourhood collaborative filtering technique. Building upon the Tapestry concept, GroupLens created an automated user based collaborative filtering technique for recommending Usenet articles that users may be interested in. The advantage of neighbourhood methods is that they are intuitive, easy to implement, and produce highly effective results \cite{survey, scalable}. Despite providing accurate recommendations, user-based collaborative filtering techniques were computed in real time and performance would degrade as more users and items were added to the system, leading to scalability and performance issues \cite{dimension, itembased, evaluationitem}.

This required collaborative filtering techniques that could easily scale and still produce high quality recommendations leading to the exploration of item-based collaborative filtering. Item-based collaborative filtering techniques were developed to address scalability limitations of the user-based techniques \cite{survey}. \citeauthor{itembased} analyzed various item-based recommendation algorithms, computing item-item similarities and comparing the accuracy with traditional KNN user based collaborative filtering techniques \cite{itembased}. \citeauthor{itembased} found that items remained fairly static in the system, whereas user behaviours and preferences would often change. Because items were found mostly static, it meant precomputation could occur for item similarities. By having precomputed item similarities, traditional item-based collaborative filtering can then be applied, thus performance and scalability would be increased \cite{scalable}.

Other techniques such as model based collaborative filtering have been investigated to overcome the performance and scalability issues. Well known model based techniques include Bayesian belief nets \cite{baysian}, clustering models \cite{clustering}, and latent semantic models \cite{latent}. These models are based on learning patterns from users previous actions to predict new items, and are expensive but can be built offline allowing high scalability. The resulting model is "very small, very fast, and essentially as accurate as nearest neighbor methods" \cite{itembased}. \citeauthor{itembased} found Bayesian networks to be practical in the context where user "preferences change slowly with respect to the model" \cite{itembased}. However, these models are not suitable for environments where the user preference model should be updated rapidly or frequently. Since model based approaches do not have to compute similarity measures to form neighbourhoods, they tend to produce faster recommendations and outperform neighbourhood models in terms of accuracy of recommendations \cite{toward, itembased}. 

Although collaborative filtering is considered to be one of the most successful approaches to recommender systems \cite{survey, toward}, they suffer from the problem of data sparsity \cite{toward, survey, itembased, koren2009matrix, koren2011, dimension}. Data sparsity is when only a small subset of user ratings on items are recorded, leading to a insufficient number of ratings to produce accurate recommendations. Data sparsity specifically tends to appear in the 'cold start' problem, where new items or new users are entered into the system, but not enough information is supplied to produce accurate recommendations since recommendations are based on common items or users \cite{survey}.

To alleviate this problem, hybrid approaches were investigated that combined collaborative filtering and other recommender techniques such as content based filtering. \citeauthor{toward} suggested creating user profiles such that demographic information could be included in similarity measures to provide extra content to find similar users or items. This effectively makes use of content-based filtering where recommendations are produced based on the content and attributes of the items, learning what attributes the user likes \cite{toward}. Well-known hybrid techniques include content-boosted collaborative filtering \cite{hybrid}, and personality diagnosis \cite{hybrid2, survey}. Hybrid approaches were implemented to address the limitations of collaborative filtering and content-based filtering techniques \cite{toward}, but have increased complexity leading to more expensive computations. Additionally, external information is needed about the content of the items which may not be available, thus making hybrid approaches impractical in certain scenarios \cite{survey}. \citeauthor{dimension} found a different approach that used dimension reduction techniques such as Singular Value Decomposition, making sparse rating models more dense by reducing the dimensionality of the product space, thereby condensing the modelled ratings of users and producing less missing information \cite{dimension}. 

In 2006, the Netflix Prize competition attracted interest in the field of recommender systems \cite{survey}. Netflix offered a \$ 1 million dollar prize to the first team to improve their movie recommender system by 10\%. This attracted interest in the research field of recommender systems. The team "BellKor in Pragmatic Chaos" won the competition in 2009 basing their solution on a combination of latent factor models and neighbourhood models \cite{winning, survey}. These models took into account many biases which improved the predication accuracy. \citeauthor{koren2009matrix} were part of the winning team, and wrote a paper explaining how temporal effects, and user biases could be accounted for in latent factor models such as Singular Value Decomposition, making it superior to neighbourhood methods \cite{koren2009matrix}. \citeauthor{koren2011} later published a paper about their findings and solutions to the Netflix Prize competition in \cite{koren2011} and \cite{winners}.


\section{Discussion of Literature Review}

It is evident that there has been an abundance of existing research on collaborative filtering techniques and ways to improve the prediction accuracy. However, \citeauthor{schafer2007collaborative} states these factors alone, do not contribute to making a good recommender system \cite{schafer2007collaborative}. Instead, \citeauthor{schafer2007collaborative} states that recommendation is not a "one-size-fits-all problem"  \cite{schafer2007collaborative}. Specific tasks, information needs, and item domains represent unique problems for recommenders, and design and evaluation of recommenders needs to be done based on the user tasks to be supported" \cite{schafer2007collaborative}. Similarly, \citeauthor{martin2009recsys} argues that the recommender algorithms is only one factor from many for providing recommendations to users. \citeauthor{martin2009recsys} explains that the user experience, data collection, and other problems which make up the whole of the recommender experience need to be considered \cite{schafer2007collaborative, martin2009recsys}. \citeauthor{interface} concluded in \cite{interface} that much of the accuracy problem has been solved in recommender systems, however delivering these accurate predictions to users in a way that creates the "best experience for them remains an open problem" \cite{interface}. 

For this reason, this project focuses on the goal of providing a recommender system that fits the needs for the "What's On The Menu" application. This involves considering how users ratings will be collected, the user experience, the recommendation process, and what factors are considered to be important in the recommendation process such as scalability, prediction accuracy, and performance.








TODO: remove this? Do I talk about this stuff here?


Existing research also focuses on the scalability and performance of these collaborative filtering techniques. In terms of scalability, the "What's On The Menu" application is not expected to contain anywhere near the number of users or items as existing recommender systems used by Netflix, Facebook, and Google. For this reason, neighbourhood methods may be a good choice, as they provide for the ability to explain the reasoning behind the recommendations, as well as give good accurate results. Scalability should not be a top concern with this application. Therefore, real time computation may be a factor to consider. 

The main focus of the recommender system will be on how to make the user experience as easy as possible for the users. This will enable the collection of user data to provide these recommendations. Another focus will be on the performance in which the recommender system can provide recommendations to the users. In this case, prediction accuracy may be less important than the speed and the performance of recommendations being produced. If the recommendation process is slow, then users will be less likely to continue using the application. On the other hand, if prediction accuracy is not very good, then users will be recommended items that they may potentially not like. A fair trade-off must be considered. 

% + main challenges, sparsity \cite{survey, dimension}, scalability, synonymy \cite{dimension}, gray sheep, shilling attacks, privacy protection, etc \cite{survey}.

\section{Representational State Transfer (REST)}

REST stands for REpresentational State Transfer. REST is an architectural style for building a software application that can be used to communicate to other systems through the HTTP protocol. It uses resources to represent important data that can be retrieved by other systems via methods from the HTTP Protocol (GET, POST, PUT, DELETE etc). It utilizes a client-server, and is stateless in the sense that all the data that is needed is sent through the HTTP protocol to the other system.

\section{Application Program Interface (API)}

API stands for Application Program Interface. An API describes the methods and ways in which others can interact and use these tools to build software applications. It specifies how software components should interact and how the software methods, or services behave. 